{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent - Boston Dataset\n",
    "Boston dataset is one of the datasets available in sklearn.\n",
    "You are given a Training dataset csv file with X train and Y train data. As studied in lecture, your task is to come up with Gradient Descent algorithm and thus predictions for the test dataset given.\n",
    "Your task is to:\n",
    "    1. Code Gradient Descent for N features and come with predictions.\n",
    "    2. Try and test with various combinations of learning rates and number of iterations.\n",
    "    3. Try using Feature Scaling, and see if it helps you in getting better results. \n",
    "Read Instructions carefully -\n",
    "    1. Use Gradient Descent as a training algorithm and submit results predicted.\n",
    "    2. Files are in csv format, you can use genfromtxt function in numpy to load data from csv file. Similarly you can use savetxt function to save data into a file.\n",
    "    3. Submit a csv file with only predictions for X test data. File name should not have spaces. File should not have any headers and should only have one column i.e. predictions. Also predictions shouldn't be in exponential form. \n",
    "    4. Your score is based on coefficient of determination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 T            V           AP           RH           EP\n",
      "count  7176.000000  7176.000000  7176.000000  7176.000000  7176.000000\n",
      "mean     19.629712    54.288154  1013.263032    73.275818   454.431293\n",
      "std       7.475256    12.751468     5.964863    14.625093    17.134571\n",
      "min       1.810000    25.360000   992.890000    25.560000   420.260000\n",
      "25%      13.470000    41.740000  1009.010000    63.202500   439.737500\n",
      "50%      20.315000    52.050000  1012.910000    74.895000   451.740000\n",
      "75%      25.720000    66.540000  1017.302500    84.925000   468.667500\n",
      "max      35.770000    81.560000  1033.300000   100.160000   495.760000\n",
      "(7176, 4)\n",
      "(7176,)\n",
      "                0            1            2            3\n",
      "count  2392.00000  2392.000000  2392.000000  2392.000000\n",
      "mean     19.71579    54.358754  1013.247216    73.408457\n",
      "std       7.38488    12.578763     5.861068    14.528135\n",
      "min       3.38000    25.360000   993.740000    26.670000\n",
      "25%      13.66000    41.730000  1009.300000    63.615000\n",
      "50%      20.45000    52.750000  1013.025000    75.090000\n",
      "75%      25.67250    66.490000  1017.172500    84.497500\n",
      "max      37.11000    80.250000  1033.290000   100.130000\n",
      "(2392, 4)\n"
     ]
    }
   ],
   "source": [
    "# Loading Datasets\n",
    "\n",
    "train_dataset = np.genfromtxt(\"../training_ccpp_x_y_train.csv\", names = True, delimiter = \",\")\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "print(train_df.describe())\n",
    "num_cols = len(train_df.columns)\n",
    "X_train = train_df.values[:, 0: num_cols -1 ]\n",
    "Y_train = train_df.values[:, num_cols - 1]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "# print(X_train)\n",
    "# print(Y_train)\n",
    "\n",
    "test_dataset = np.genfromtxt(\"../test_ccpp_x_test.csv\", names = None, delimiter = \",\")\n",
    "test_df = pd.DataFrame(test_dataset)\n",
    "print(test_df.describe())\n",
    "num_cols = len(test_df.columns)\n",
    "X_test = test_df.values\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "def train_n_models(X_train, Y_train, X_test):\n",
    "    n = 3\n",
    "    sub_Size = 1\n",
    "    Y_pred = np.zeros(len(X_test))\n",
    "    Y_train_pred = np.zeros(int(sub_Size * len(X_train)))\n",
    "    num_s = 0\n",
    "    for i in range(n):\n",
    "        X1, X2, Y1, Y2 = model_selection.train_test_split(X_train, Y_train, test_size = 1 - sub_Size)\n",
    "        model = GradientBoostingRegressor(ls = 'huber', subsample = 0.95)\n",
    "        model.fit(X1, Y1)\n",
    "        #print(\"Training Score : \", model.score(X_train, Y_train))\n",
    "        score = model.score(X1, Y1)\n",
    "        if score >= 0:\n",
    "            print(num_s,\" Training Score : \", score)\n",
    "            Y_i_pred = model.predict(X_test)\n",
    "            Y_pred += Y_i_pred\n",
    "            Y_train_i_pred = model.predict(X1)\n",
    "            Y_train_pred += Y_train_i_pred\n",
    "            num_s += 1\n",
    "    Y_pred /= num_s\n",
    "    Y_train_pred /= num_s          \n",
    "    \n",
    "    #print(Y_pred, pred)\n",
    "    return Y_train_pred, Y_pred\n",
    "\n",
    "def add_more_features (X_train, imp_cols_indices = []):\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    num_f = len(X_train_df.columns)\n",
    "    col_names = X_train_df.columns\n",
    "    \n",
    "    if len(imp_cols_indices) == 0 :\n",
    "        imp_cols_indices = np.arange(num_f)\n",
    "    new_df = X_train_df.copy()\n",
    "    num_imp_cols = len(imp_cols_indices)\n",
    "    \n",
    "    pow = 3\n",
    "    \n",
    "    for i1 in range(num_imp_cols) :\n",
    "        i = imp_cols_indices[i1]\n",
    "        new_df[ str(col_names[i]) + \"_pow_\" + str(pow) ] = X_train_df[i] ** 3\n",
    "\n",
    "    for i1 in range(num_imp_cols) :\n",
    "        for j1 in range(i1, num_imp_cols):\n",
    "            i = imp_cols_indices[i1]\n",
    "            j = imp_cols_indices[j1]\n",
    "            new_df[ str(col_names[i]) + \"_\" + str(col_names[j])] = X_train_df[i] * X_train_df[j]\n",
    "            new_df[ str(col_names[i])*2 + \"_\" + str(col_names[j])] = X_train_df[i]**2 * X_train_df[j]\n",
    "            new_df[ str(col_names[i]) + \"_\" + str(col_names[j])*2 ] = X_train_df[i] * X_train_df[j]**2\n",
    "            new_df[ str(col_names[i])*2 + \"_\" + str(col_names[j])*2 ] = X_train_df[i]**2 * X_train_df[j]**2\n",
    "            \n",
    "    \n",
    "    return new_df.values\n",
    "\n",
    "def add_cube_features (X_train):\n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    num_f = len(X_train_df.columns)\n",
    "    col_names = X_train_df.columns\n",
    "    \n",
    "\n",
    "    imp_cols_indices = np.arange(num_f)\n",
    "    new_df = X_train_df.copy()\n",
    "    num_imp_cols = len(imp_cols_indices)\n",
    "    \n",
    "    pow = 3\n",
    "    \n",
    "    for i1 in range(num_imp_cols) :\n",
    "        i = imp_cols_indices[i1]\n",
    "        new_df[ str(col_names[i]) + \"_pow_\" + str(pow) ] = X_train_df[i] ** 3\n",
    "    \"\"\"\n",
    "    \n",
    "    for i1 in range(num_imp_cols) :\n",
    "        for j1 in range(i1, num_imp_cols):\n",
    "            i = imp_cols_indices[i1]\n",
    "            j = imp_cols_indices[j1]\n",
    "            new_df[ str(col_names[i]) + \"_\" + str(col_names[j])] = X_train_df[i] * X_train_df[j]\n",
    "    \"\"\"\n",
    "    #print(new_df.describe())\n",
    "    return new_df.values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7176, 48)\n",
      "(2392, 48)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing as pps\n",
    "\n",
    "X_train_new = add_more_features(X_train)\n",
    "X_test_new = add_more_features(X_test)\n",
    "\n",
    "#X_train_new = add_more_features(X_train_new)\n",
    "#X_test_new = add_more_features(X_test_new)\n",
    "\n",
    "std_scaler = pps.StandardScaler()\n",
    "std_scaler.fit(X_train_new)\n",
    "\n",
    "X_train_scaled = std_scaler.transform(X_train_new)\n",
    "X_test_scaled = std_scaler.transform(X_test_new)\n",
    "\n",
    "import copy\n",
    "X_train_original = copy.deepcopy(X_train)\n",
    "X_test_original = copy.deepcopy(X_test)\n",
    "\n",
    "X_train = X_train_scaled\n",
    "X_test = X_test_scaled\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7176, 20)\n",
      "(2392, 20)\n"
     ]
    }
   ],
   "source": [
    "# extract best features\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(20)\n",
    "pca.fit(np.concatenate((X_train, X_test), axis = 0))\n",
    "\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Training Score :  0.95302892316\n",
      "1  Training Score :  0.953009801315\n",
      "2  Training Score :  0.952794772998\n"
     ]
    }
   ],
   "source": [
    "Y_train_pred, Y_pred = train_n_models(X_train, Y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(score(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"ccpp_dataset_pred.csv\", Y_pred, '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
