{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lalit Jain\n",
    "## jain.lalit98@outlook.com\n",
    "\n",
    "## Text Classification Using Naive Bayes\n",
    "### Your task is to:\n",
    "    1. Perform Test Classification using Multinomial Naive Bayes(already implemented in sklearn).\n",
    "    2. Implement Naive Bayes on your own from scratch for text classification. \n",
    "    3. Compare Results of your implementation of Naive Bayes with one in Sklearn.\n",
    "#### Dataset - \n",
    "    http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n",
    "#### Comments : \n",
    "    Your code must have proper comments for better understanding.\n",
    "#### Score : \n",
    "    Score will be given by the TA based on your submission.\n",
    "#### Submission : \n",
    "    You have to upload zipped file which has python notebook with implementation and dataset used.\n",
    "#### Your project will be evaluated on following parameters -\n",
    "    1. Correctness of Code - Own Implementation Naive Bayes (Max Score 50)\n",
    "    2. Comparison (Max Score 10)\n",
    "    3. Commenting (Max Score 10)\n",
    "    4. Correctness of Code - Sklearn Naive Bayes (Max Score 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'headers_info.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cdbe03a437f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"headers_info.txt\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mheaders_file\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'headers_info.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# Data Cleaning\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# headers_list contains header words like Xref, Path, From etc. \n",
    "# colected them from some of the files\n",
    "\n",
    "headers_list = []\n",
    "\n",
    "s = \"\"\n",
    "\n",
    "with open(\"headers_info.txt\") as headers_file :\n",
    "    s += ''.join(headers_file.readlines())\n",
    "\n",
    "words = s.split(' ')\n",
    "\n",
    "for word in words :\n",
    "    if re.search(\"\\w\\:\", word) != None:\n",
    "        headers_list.append(word)\n",
    "# minor correction\n",
    "headers_list[headers_list.index('Writeto:')] = 'Write to:'\n",
    "print(headers_list)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "the function takes a string corpus of a single document, and the headers_list\n",
    "Steps in preprocessing:\n",
    "    1. remove sentences containing headers\n",
    "    2. remove numbers, punctuations\n",
    "    3. remove stopwords: imported lists of stopwords from 3 libraries\n",
    "    \n",
    "\"\"\"\n",
    "def preprocess_corpus(corpus, headers_list, remove_headers = True):\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "    ENGLISH_STOP_WORDS_LIST = list(ENGLISH_STOP_WORDS)\n",
    "    STOP_WORDS_LIST = list(STOP_WORDS)\n",
    "\n",
    "    #final list of stop words by merging lists from all three sources\n",
    "    stop_words = list(set(stopwords.words('english') + ENGLISH_STOP_WORDS_LIST + STOP_WORDS_LIST))\n",
    "    \n",
    "    #print(stop_words)\n",
    "    \n",
    "    if remove_headers :    \n",
    "        \"\"\"\n",
    "        identify lines containing a header and remove them\n",
    "        \"\"\"\n",
    "        headers_removed_corpus = []\n",
    "        for line in corpus:\n",
    "            #line = line.lower()\n",
    "            line = line.strip()\n",
    "            if line == '' :\n",
    "                continue\n",
    "            is_header = False\n",
    "            for header in headers_list:\n",
    "                if header in line:\n",
    "                    is_header = True\n",
    "            if not is_header :\n",
    "                headers_removed_corpus.append(line)\n",
    "            #else:\n",
    "            #    print(line)\n",
    "        #pprint(headers_removed_corpus)\n",
    "    else:\n",
    "        headers_removed_corpus = corpus \n",
    "    \"\"\"\n",
    "    to_english removes non-english words from input string\n",
    "    \"\"\"\n",
    "    import string    \n",
    "    def to_english(s):\n",
    "        # remove extra spaces\n",
    "        arr = re.sub('\\s', ' ', s)\n",
    "        arr = arr.split(' ')\n",
    "        retval = []\n",
    "        for word in arr:\n",
    "            #check word for alphanumeric characters by removing punctuations\n",
    "            if word.translate(string.punctuation).isalnum() : \n",
    "                retval.append(word.strip())\n",
    "        return ' '.join(retval)\n",
    "\n",
    "    \"\"\"\n",
    "    Removing non english words and punctuations\n",
    "    \"\"\"\n",
    "    non_english_and_punct_removed = []\n",
    "    for line in headers_removed_corpus:\n",
    "        clean_line = []\n",
    "        #print(line)\n",
    "        #remove punctuations using regex\n",
    "        line = re.sub(\"[:,-]\", ' ', line) \n",
    "        line = re.sub(\"[!\\\"#$%&\\'()\\*\\+,\\-\\./:;<=>?@\\[\\\\\\]^_`{|}~]\", ' ', line) #,'!\"#$%&()*,-.:;<=>?@^_`{|}~'\n",
    "        #print(line)\n",
    "        words = line.split(' ')\n",
    "        for word in words:\n",
    "            clean_line.append(to_english(word).strip())\n",
    "        clean_line = (' '.join(clean_line)).strip()\n",
    "        # remove extra spaces\n",
    "        clean_line = re.sub('\\s +', ' ', clean_line)\n",
    "        #filter empty strings\n",
    "        if clean_line != '':\n",
    "            non_english_and_punct_removed.append(clean_line)\n",
    "    #pprint(non_english_and_punct_removed)\n",
    "\n",
    "    \"\"\"\n",
    "    remove stopwords\n",
    "    \"\"\"\n",
    "    stop_words_removed = []\n",
    "    \n",
    "    for line in non_english_and_punct_removed:\n",
    "        words = line.split(' ')\n",
    "        new_line = []\n",
    "        for word in words:\n",
    "            # remove numbers\n",
    "            word = re.sub(\"[0-9]+\", '', word)\n",
    "            # remove extra spaces\n",
    "            word = re.sub(\"\\s\", ' ', word)\n",
    "            #convert word to lawer case\n",
    "            word = word.strip().lower()\n",
    "            if word == '' :\n",
    "                continue\n",
    "            if word not in stop_words:\n",
    "                new_line.append(word)\n",
    "        new_line = ' '.join(new_line)\n",
    "        if new_line != '' :\n",
    "            stop_words_removed.append(new_line)\n",
    "    #pprint(stop_words_removed)\n",
    "    \n",
    "    final_data = '.'.join(stop_words_removed)\n",
    "    return final_data\n",
    "\n",
    "\"\"\"\n",
    "dummy function to find paths of all files in data files folder\n",
    "\"\"\"\n",
    "def find_all_paths():\n",
    "    document_paths = []\n",
    "    from pprint import pprint\n",
    "    import os\n",
    "    walk = os.walk('.\\\\20_newsgroups', topdown = False)\n",
    "    for root, dirs, files in walk :\n",
    "        for file in files:\n",
    "            doc = {}\n",
    "            doc['root'] = root # path to folder containing the data files\n",
    "            doc['file'] = file # file name\n",
    "            document_paths.append(doc)\n",
    "    return document_paths\n",
    "#pprint(document_paths[0:100])\n",
    "\n",
    "\"\"\"\n",
    "dummy function to preprocess all the data files and create new clean data files\n",
    "\n",
    "\"\"\"\n",
    "def clean_all_docs(document_paths, remove_headers = True) :\n",
    "    corpus = \"\"\n",
    "    i = 0\n",
    "    import time\n",
    "    st = time.time()\n",
    "    new_paths = []\n",
    "    for doc_path in document_paths:\n",
    "        #if i == 2 :\n",
    "        #    break\n",
    "        path = doc_path['root'] + \"\\\\\" + doc_path['file']\n",
    "        with open(path) as doc :\n",
    "            data = doc.readlines()\n",
    "            i += 1\n",
    "            # preprocess data of currnt file\n",
    "            clean_corpus = preprocess_corpus(data, headers_list, remove_headers)\n",
    "            \n",
    "            clean_data_file_root = doc_path['root'].replace('.\\\\','.\\\\clean_data\\\\')\n",
    "            \n",
    "            # create directory if does not already exist\n",
    "            os.makedirs(clean_data_file_root, exist_ok = True)\n",
    "            # actual path of clean data file \n",
    "            clean_data_file_path = clean_data_file_root + \"\\\\\" +doc_path['file'] + '.txt'\n",
    "            # write to clean data file\n",
    "            with open(clean_data_file_path, 'wb') as file_clean_data :\n",
    "                file_clean_data.write(bytes(clean_corpus,'utf8'))\n",
    "                file_clean_data.close()\n",
    "            doc.close()\n",
    "            \n",
    "            #verbose\n",
    "            if i % 1000 == 0 :\n",
    "                print( i, \"Files Processed in\", round(time.time() - st, 3), \"sec\")\n",
    "                st = time.time()\n",
    "\n",
    "\n",
    "document_paths = find_all_paths()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Option to remove headers since for this dataset, removing headers actually decreases accuracy of classifier\n",
    "\n",
    "\"\"\"\n",
    "clean_all_docs(document_paths, remove_headers = True)\n",
    "#pprint(new_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from nltk.corpus import words as nltk_words\n",
    "\n",
    "#english dictionary contains english words from NLTK, dictionary for fast access\n",
    "english_dictionary = {word : True for word in list(set(nltk_words.words()))}\n",
    "\n",
    "\"\"\"\n",
    "check if input word is an english word \n",
    "\"\"\"\n",
    "\n",
    "def is_in_english(word):\n",
    "    try:\n",
    "        return english_dictionary[word]\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\"\"\"\n",
    "Build Dictionary of Words\n",
    "\"\"\"\n",
    "\n",
    "# document paths stores paths to all clean data files \n",
    "document_paths = []\n",
    "\n",
    "# pretty print module for better output format\n",
    "from pprint import pprint\n",
    "import os\n",
    "# find all file paths in clean data folder\n",
    "walk = os.walk('.\\\\clean_data\\\\20_newsgroups', topdown = False)\n",
    "for root, dirs, files in walk :\n",
    "    for file in files:\n",
    "        doc = {}\n",
    "        dir_ = root.split('\\\\')[-1]\n",
    "        doc['path'] = root+\"\\\\\"+file\n",
    "        doc['target'] = dir_\n",
    "        document_paths.append(doc)\n",
    "#pprint(document_paths[0:10000:100])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(document_paths)\n",
    "#df.head()\n",
    "\n",
    "# targets is list of all class names\n",
    "targets = list(set(df['target'].values))\n",
    "pprint(targets)\n",
    "\n",
    "# class_dict contains indices mapped to actual class names\n",
    "class_dict = { i : targets[i] for i in range(len(targets))}\n",
    "pprint(class_dict)\n",
    "\n",
    "# class is numeric representation for class name\n",
    "# categry is the actual class name\n",
    "class_df = pd.DataFrame()\n",
    "class_df['class'] = class_dict\n",
    "class_df['category'] = [class_dict[key] for key in class_dict]\n",
    "class_df.to_csv(\"class_dict.csv\", index = False)\n",
    "\n",
    "df['class'] = [targets.index(target) for target in df['target'].values]\n",
    "\n",
    "df.head()\n",
    "\n",
    "X = df['path']\n",
    "Y = df['class']\n",
    "print(X[0:5])\n",
    "print(Y[0:5])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Split the final data files into training and testing datasets(actually containing the file paths)\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 0) \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['path'] = X_train\n",
    "train_df['class'] = Y_train\n",
    "\n",
    "#save file storing the paths to train files\n",
    "train_df.to_csv(\"train_files.csv\", index = False)\n",
    "\n",
    "#save file storing the paths to test files\n",
    "test_df = pd.DataFrame()\n",
    "test_df['path'] = X_test\n",
    "test_df['class'] = Y_test\n",
    "test_df.to_csv(\"test_files.csv\", index = False)\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "test_df.head()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Build Master Dictionary that is used to decide the features \n",
    "\n",
    "master_dictionary is mapping of each english word with its count\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "master_dictionary = {}\n",
    "\n",
    "i = 0\n",
    "import time\n",
    "st = time.time()\n",
    "new_paths = []\n",
    "for doc_path in X_train:\n",
    "    #if i == 1 :\n",
    "    #    break\n",
    "    path = doc_path\n",
    "    with open(path) as doc :\n",
    "        num_tokens_in_doc = 0\n",
    "        data = ''.join(doc.readlines()).split('.')\n",
    "        for line in data:\n",
    "            for word in line.split():\n",
    "                # non english words can't be used as features for this dataset\n",
    "                if not is_in_english(word):\n",
    "                    continue\n",
    "                #print(word)\n",
    "                # using try/catch approach for very fast search\n",
    "                try:\n",
    "                    master_dictionary[word] += 1\n",
    "                except KeyError:\n",
    "                    master_dictionary[word] = 1\n",
    "                    continue\n",
    "        doc.close()\n",
    "        i += 1\n",
    "        # verbose\n",
    "        if i % 1000 == 0 :\n",
    "            print( i, \"Files Processed in\", round((time.time() - st), 3) , \"sec\")\n",
    "            st = time.time()\n",
    "#print(master_dictionary)\n",
    "\n",
    "import numpy as np\n",
    "keys = list(master_dictionary)\n",
    "freq = np.array([ master_dictionary[key] for key in keys])\n",
    "\n",
    "words_df = pd.DataFrame()\n",
    "words_df['word'] = keys\n",
    "words_df['frequency'] = freq\n",
    "\n",
    "# all words csv contains master dictionary\n",
    "words_df.to_csv('all_words.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load master dictionary\n",
    "words_df = pd.DataFrame(pd.read_csv(\"all_words.csv\"))\n",
    "\n",
    "words_df.head()\n",
    "\n",
    "all_words = words_df['word']\n",
    "freq = words_df['frequency']\n",
    "\n",
    "# rebuild master dictionary object\n",
    "master_dictionary = {all_words[i] : freq[i] for i in range(len(freq))}\n",
    "\n",
    "print(max(freq))\n",
    "\n",
    "# feature frequency range to be set here\n",
    "max_allowed_freq = max(freq)\n",
    "min_allowed_freq = 15\n",
    "\n",
    "# plot histogram just for fun\n",
    "filtered_freq = [f for f in freq if (f <= max_allowed_freq and f >= min_allowed_freq)]\n",
    "print(len(filtered_freq))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(filtered_freq, 50)\n",
    "plt.show()\n",
    "\n",
    "# ignore\n",
    "too_common_words = [word for word in master_dictionary if master_dictionary[word] > max_allowed_freq]\n",
    "print(\"No. of common words : \", len(too_common_words))\n",
    "\n",
    "\n",
    "# ignore\n",
    "too_rare_words = [word for word in master_dictionary if master_dictionary[word] < min_allowed_freq]\n",
    "print(\"No. of rare words : \", len(too_rare_words))\n",
    "#print(too_rare_words)\n",
    "\n",
    "\"\"\"\n",
    "Visualize the range of frequencies of selected words \n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "Y_freq = freq[freq >= min_allowed_freq]\n",
    "Y_freq = Y_freq[Y_freq <= max_allowed_freq]\n",
    "plt.plot(np.sort(Y_freq))\n",
    "plt.show()\n",
    "\n",
    "selected_words = [word for word in all_words \n",
    "                  if master_dictionary[word] >= min_allowed_freq \n",
    "                  and master_dictionary[word] <= max_allowed_freq \n",
    "                  and type(word) == str \n",
    "                  and len(word) > 1]\n",
    "\n",
    "#for word in keys:\n",
    "#    if freq\n",
    "print(len(selected_words))\n",
    "print(selected_words)\n",
    "\n",
    "selected_words_freq = [master_dictionary[word] for word in selected_words]\n",
    "plt.plot(np.sort(selected_words_freq))\n",
    "plt.show()\n",
    "\n",
    "selected_words_df = pd.DataFrame()\n",
    "selected_words_df['word'] = selected_words\n",
    "selected_words_df['frequency'] = selected_words_freq\n",
    "\n",
    "\"\"\"\n",
    "selected words are words chosen as features from the master dictionary\n",
    "\"\"\"\n",
    "selected_words_df.to_csv('selected_words.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Build Dataset of word counts as features\n",
    "\"\"\"\n",
    "def prepare_dataset(selected_words_file_path, data_files_path):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # load class dict\n",
    "    class_dict_df = pd.DataFrame(pd.read_csv(\"class_dict.csv\"))\n",
    "    #class_dict_df.head()\n",
    "\n",
    "    # load selected words dictionary\n",
    "    #selected_words_df = pd.DataFrame(pd.read_csv('all_words.csv'))\n",
    "    selected_words_df = pd.DataFrame(pd.read_csv(selected_words_file_path))\n",
    "    #selected_words_df.head()\n",
    "\n",
    "    selected_words = selected_words_df['word'].values\n",
    "    \n",
    "    selected_words_dictionary = {word : True for word in selected_words}\n",
    "    \n",
    "    \n",
    "    #number of selected words\n",
    "    num_selected_words = len(selected_words)\n",
    "\n",
    "    #load file containing document paths\n",
    "    data_file_paths_df = pd.DataFrame(pd.read_csv(data_files_path))\n",
    "    #data_file_paths_df.head()\n",
    "\n",
    "    data_file_paths = data_file_paths_df['path'].values\n",
    "    \n",
    "    #directly used as target Y\n",
    "    data_file_class = data_file_paths_df['class'].values\n",
    "\n",
    "    import time\n",
    "    matrix = []\n",
    "    st = time.time()\n",
    "    for i in range(len(data_file_paths)) :\n",
    "        #if i == 2000:\n",
    "        #    break\n",
    "    \n",
    "        if (i+1) % 1000 == 0 :\n",
    "            et = time.time()\n",
    "            print( i+1, \"Files Processed in\", round((time.time() - st), 3) , \"sec\")\n",
    "            st = time.time()    \n",
    "        path = data_file_paths[i]\n",
    "        target = data_file_class[i]\n",
    "        with open(path) as file :\n",
    "            X = []\n",
    "            count = {}\n",
    "            data = file.readlines()\n",
    "            words_in_file = (''.join(data)).split(' ')\n",
    "            for word in words_in_file:\n",
    "                try:\n",
    "                    if selected_words_dictionary[word]:\n",
    "                        # try catch approach for fast search\n",
    "                        try:\n",
    "                            count[word] += 1\n",
    "                        except:\n",
    "                            count[word] = 1\n",
    "                except:\n",
    "                    continue\n",
    "            for word in selected_words:\n",
    "                # try catch approach for fast search\n",
    "                try :\n",
    "                    X = count[word] \n",
    "                except :\n",
    "                    count[word] = 0 \n",
    "            #print(X)\n",
    "            X = [count[word] for word in selected_words if word in count] \n",
    "            #print(len(X))\n",
    "            matrix.append(X)\n",
    "            #X_train_df.iloc[i, :] = \n",
    "            file.close()\n",
    "    #print(len(matrix))\n",
    "    #print(len(matrix[0]))\n",
    "\n",
    "    X_df = pd.DataFrame(matrix, columns = selected_words)\n",
    "\n",
    "    #X_df.describe()\n",
    "\n",
    "    import copy\n",
    "    X = X_df.values\n",
    "    Y = copy.deepcopy(data_file_class)\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    dataset_df = pd.DataFrame(X_df)\n",
    "    dataset_df['target'] = Y\n",
    "    return dataset_df\n",
    "\n",
    "train_files_path = \"train_files.csv\"\n",
    "test_files_path = \"test_files.csv\"\n",
    "\n",
    "selected_words_file_path = 'selected_words.csv'\n",
    "\n",
    "train_dataset_df = prepare_dataset(selected_words_file_path, train_files_path)\n",
    "train_dataset_df.to_csv(\"20_newsgroups_dataset_min_freq_15.csv\", index = False)\n",
    "\n",
    "test_dataset_df = prepare_dataset(selected_words_file_path, test_files_path)\n",
    "test_dataset_df.to_csv(\"20_newsgroups_test_dataset_min_freq_15.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load training dataset\n",
    "import time\n",
    "st = time.time()\n",
    "train_dataset_df = pd.read_csv(\"20_newsgroups_dataset_min_freq_15.csv\")\n",
    "et = time.time()\n",
    "\n",
    "print(\"Loading Time for Training Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "train_dataset_df.describe()\n",
    "\n",
    "X_train_df = train_dataset_df.drop('target', axis = 1) \n",
    "Y_train_df = train_dataset_df['target']\n",
    "\n",
    "# X_train_df.describe()\n",
    "\n",
    "# Y_train_df.describe()\n",
    "\n",
    "# load testing dataset\n",
    "import time\n",
    "st = time.time()\n",
    "test_dataset_df = pd.read_csv(\"20_newsgroups_test_dataset_min_freq_15.csv\")\n",
    "et = time.time()\n",
    "\n",
    "print(\"Loading Time for Test Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "test_dataset_df.describe()\n",
    "\n",
    "X_test_df = test_dataset_df.drop('target', axis = 1)\n",
    "Y_test_df = test_dataset_df['target']\n",
    "\n",
    "# X_test_df.describe()\n",
    "\n",
    "# Y_test_df.describe()\n",
    "\n",
    "words = X_train_df.columns\n",
    "#print(words)\n",
    "possible_classes = list(set(Y_train_df.values))\n",
    "#print(possible_classes)\n",
    "\n",
    "# predict class for single document\n",
    "def predict_single(X, model) :\n",
    "    \n",
    "    # initial max class probablity, and assigned class\n",
    "    max_class_prob = -np.inf\n",
    "    max_class = None\n",
    "    # all possible classes\n",
    "    possible_classes = model.keys()\n",
    "    # find probablities of this document belonging to each of the possible classes \n",
    "    for y in possible_classes :\n",
    "        # convert input array to numpy array in case it isn't\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # assign class prior\n",
    "        prob_X_equals_x_given_Y_equals_y = model[y]['class_prior']\n",
    "        # multinomial feature probability addition to get probability \n",
    "        # that this document belongs to current class\n",
    "        prob_X_equals_x_given_Y_equals_y += (X * model[y]['log_prob_sum']).sum()\n",
    "        \n",
    "        # if prob is max update max_class variables\n",
    "        if prob_X_equals_x_given_Y_equals_y > max_class_prob :\n",
    "            max_class_prob = prob_X_equals_x_given_Y_equals_y\n",
    "            max_class = y\n",
    "    return max_class\n",
    "\n",
    "\"\"\"\n",
    "Internal function\n",
    "generate_model takes as input a dictionary and \n",
    "a tuning parameter alpha that accounts for probability correction(similar to the inbuilt classifer)\n",
    "\"\"\"\n",
    "def generate_model(dictionary, alpha) :\n",
    "    model = {}\n",
    "    possible_classes = dictionary['possible_classes']\n",
    "    num_words = dictionary[\"vocabulary_size\"]\n",
    "    \n",
    "    range_words = range(num_words)\n",
    "    \n",
    "    for y in possible_classes :\n",
    "        #class_prior\n",
    "        model[y] = {}\n",
    "        prob_Y_equals_y = np.log(dictionary[y][\"class_count\"]/dictionary[\"total_data\"])\n",
    "        prob_X_equals_x_given_Y_equals_y = 0 \n",
    "        prob_X_equals_x_given_Y_equals_y += prob_Y_equals_y\n",
    "        total_words_in_class_y_docs = dictionary[y][\"total_words\"]        \n",
    "        count_f_class_y = np.array([dictionary[y][f] for f in range_words])\n",
    "        prob_f_class_y = (count_f_class_y+alpha)/(total_words_in_class_y_docs + alpha*num_words) \n",
    "        \n",
    "        model[y]['class_prior'] = prob_Y_equals_y\n",
    "        model[y]['log_prob_sum'] = np.log(prob_f_class_y)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def predict(X, model) :\n",
    "    #if not isinstance(X[0], list) :\n",
    "    #    return None #predict_single(X, model)\n",
    "    Y_pred = []\n",
    "    for x in X :\n",
    "        Y_pred.append(predict_single(x, model))\n",
    "    return Y_pred\n",
    "\n",
    "\"\"\"\n",
    "fit function takes as input the trainig dataset and alpha, the tuning parameter\n",
    "\"\"\"\n",
    "def fit(X, Y, alpha) :\n",
    "    # check if alpha is in range of [0, 1]\n",
    "    # if not set alpha to 1\n",
    "    if alpha > 1 or alpha < 0 :\n",
    "        print(\"Alpha parameter not in range [0,1]...\")\n",
    "        print(\"setting alpha = 1\")\n",
    "        alpha = 1\n",
    "    num_words = len(X[0])\n",
    "    possible_classes = list(set(Y))\n",
    "    \n",
    "    #dictionary with keys as possible classes, total number of training documents, vocabulary size\n",
    "    dictionary = {}\n",
    "    dictionary[\"total_data\"] = len(Y)\n",
    "    dictionary[\"vocabulary_size\"] = num_words\n",
    "    dictionary[\"possible_classes\"] = possible_classes\n",
    "    \n",
    "    # build internal dictionaries for each of the possible classes\n",
    "    # dict mapped to each class contains keys: \n",
    "    # total words and \n",
    "    # a class count that is the total number of training documents belonging to current class \n",
    "    \n",
    "    for y in possible_classes :\n",
    "        y_dict = {i : X[Y == y, i].sum() for i in range(num_words)}\n",
    "        y_dict['total_words'] = sum(y_dict.values())\n",
    "        y_dict['class_count'] = sum(Y == y)\n",
    "        #print(y_dict['total_count'])\n",
    "        dictionary[y] = y_dict\n",
    "    # call is made to generate model function that uses the \"dictionary\" and \n",
    "    # alpha \n",
    "    # to find effect on probabilities of a document belonging to a particular class\n",
    "    # separate function for simplicity\n",
    "    return generate_model(dictionary, alpha)\n",
    "\n",
    "# fit the model\n",
    "import time\n",
    "st = time.time()\n",
    "model = fit(X_train_df.values, Y_train_df.values, 0.01)\n",
    "et = time.time()\n",
    "\n",
    "print(\"Training Time:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "#print(model)\n",
    "\n",
    "\"\"\"\n",
    "Train Data Evaluation\n",
    "\"\"\"\n",
    "import time\n",
    "st = time.time()\n",
    "Y_train_pred = predict(X_train_df.values, model)\n",
    "et = time.time()\n",
    "\n",
    "print(\"Prediction Time for Training Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "#print(Y_train_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"Confusion Matrix for Training Dataset\")\n",
    "pprint(confusion_matrix(Y_train_pred, Y_train_df.values))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"classification Report for Training Dataset\")\n",
    "print(classification_report(Y_train_pred, Y_train_df.values))\n",
    "\n",
    "\"\"\"\n",
    "Test Data Evaluation\n",
    "\"\"\"\n",
    "import time\n",
    "st = time.time()\n",
    "Y_test_pred = predict(X_test_df.values, model)\n",
    "et = time.time()\n",
    "\n",
    "print(\"Prediction Time for Testing Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "training_time = (et-st)\n",
    "print(training_time)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"Confusion Matrix for Test Dataset\")\n",
    "pprint(confusion_matrix(Y_test_pred, Y_test_df.values))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"classification Report for Test Dataset\")\n",
    "print(classification_report(Y_test_pred, Y_test_df.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "sklearn_model = MultinomialNB(alpha = 0.01)\n",
    "sklearn_model.fit(X_train_df.values, Y_train_df.values)\n",
    "et = time.time()\n",
    "print(\"Training Time:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "Y_train_pred = sklearn_model.predict(X_train_df.values)\n",
    "et = time.time()\n",
    "print(\"Prediction Time for Training Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "print(\"Confusion Matrix for Training Dataset\")\n",
    "pprint(confusion_matrix(Y_train_pred, Y_train_df.values))\n",
    "\n",
    "print(\"classification Report for Training Dataset\")\n",
    "print(classification_report(Y_train_pred, Y_train_df.values))\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "Y_test_pred = sklearn_model.predict(X_test_df.values)\n",
    "et = time.time()\n",
    "print(\"Prediction Time for Test Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "print(\"Confusion Matrix for Test Dataset\")\n",
    "pprint(confusion_matrix(Y_test_pred, Y_test_df.values))\n",
    "\n",
    "print(\"classification Report for Test Dataset\")\n",
    "print(classification_report(Y_test_pred, Y_test_df.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of results obtained from own implementation and the inbuilt classifier in sklearn \n",
    "\n",
    "(referring to inbuilt classifier as MultinomialNB)\n",
    "\n",
    "1. With the same dataset and alpha parameter, MultinomialNB and my own implementation give exactly the same results.\n",
    "2. Tuning parameter alpha should be kept low for better accuracy.\n",
    "2. The speed of MultinomialNB is around 10 times, since its implemented with cython.\n",
    "3. Without removing headers, I got an F1 Score of 0.89(but using all words as features), MultinomialNB performed likewise.\n",
    "4. Increasing the number of features boosts the score of this claasifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
