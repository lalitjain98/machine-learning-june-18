{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lalit Jain\n",
    "## jain.lalit138@gmail.com\n",
    "\n",
    "## Text Classification Using Naive Bayes\n",
    "\n",
    "## Dataset - \n",
    "    http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Xref:', 'Path:', 'From:', 'Newsgroups:', 'Subject:', 'FAQ:', 'Message-ID:', 'Date:', 'Expires:', 'Followup-To:', 'Distribution:', 'Organization:', 'Approved:', 'Supersedes:', 'Archive-name:', 'Alt-atheism-archive-name:', 'Last-modified:', 'Write to:', 'Summary:', 'Keywords:', 'Version:', 'Lines:', 'Telephone:', 'or:', 'Telephone:', 'Fax:']\n",
      "1000 Files Processed in 12.929 sec\n",
      "2000 Files Processed in 13.013 sec\n",
      "3000 Files Processed in 12.458 sec\n",
      "4000 Files Processed in 11.383 sec\n",
      "5000 Files Processed in 10.514 sec\n",
      "6000 Files Processed in 12.321 sec\n",
      "7000 Files Processed in 11.028 sec\n",
      "8000 Files Processed in 10.786 sec\n",
      "9000 Files Processed in 10.201 sec\n",
      "10000 Files Processed in 10.715 sec\n",
      "11000 Files Processed in 12.273 sec\n",
      "12000 Files Processed in 11.867 sec\n",
      "13000 Files Processed in 10.723 sec\n",
      "14000 Files Processed in 11.983 sec\n",
      "15000 Files Processed in 13.097 sec\n",
      "16000 Files Processed in 14.424 sec\n",
      "17000 Files Processed in 12.263 sec\n",
      "18000 Files Processed in 15.06 sec\n",
      "19000 Files Processed in 15.039 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# Data Cleaning\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "# headers_list contains header words like Xref, Path, From etc. \n",
    "# colected them from some of the files\n",
    "\n",
    "headers_list = []\n",
    "\n",
    "s = \"\"\n",
    "\n",
    "with open(\"headers_info.txt\") as headers_file :\n",
    "    s += ''.join(headers_file.readlines())\n",
    "\n",
    "words = s.split(' ')\n",
    "\n",
    "for word in words :\n",
    "    if re.search(\"\\w\\:\", word) != None:\n",
    "        headers_list.append(word)\n",
    "# minor correction\n",
    "headers_list[headers_list.index('Writeto:')] = 'Write to:'\n",
    "print(headers_list)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "the function takes a string corpus of a single document, and the headers_list\n",
    "Steps in preprocessing:\n",
    "    1. remove sentences containing headers\n",
    "    2. remove numbers, punctuations\n",
    "    3. remove stopwords: imported lists of stopwords from 3 libraries\n",
    "    \n",
    "\"\"\"\n",
    "def preprocess_corpus(corpus, headers_list, remove_headers = True):\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "    ENGLISH_STOP_WORDS_LIST = list(ENGLISH_STOP_WORDS)\n",
    "    STOP_WORDS_LIST = list(STOP_WORDS)\n",
    "\n",
    "    #final list of stop words by merging lists from all three sources\n",
    "    stop_words = list(set(stopwords.words('english') + ENGLISH_STOP_WORDS_LIST + STOP_WORDS_LIST))\n",
    "    \n",
    "    #print(stop_words)\n",
    "    \n",
    "    if remove_headers :    \n",
    "        \"\"\"\n",
    "        identify lines containing a header and remove them\n",
    "        \"\"\"\n",
    "        headers_removed_corpus = []\n",
    "        for line in corpus:\n",
    "            #line = line.lower()\n",
    "            line = line.strip()\n",
    "            if line == '' :\n",
    "                continue\n",
    "            is_header = False\n",
    "            for header in headers_list:\n",
    "                if header in line:\n",
    "                    is_header = True\n",
    "            if not is_header :\n",
    "                headers_removed_corpus.append(line)\n",
    "            #else:\n",
    "            #    print(line)\n",
    "        #pprint(headers_removed_corpus)\n",
    "    else:\n",
    "        headers_removed_corpus = corpus \n",
    "    \"\"\"\n",
    "    to_english removes non-english words from input string\n",
    "    \"\"\"\n",
    "    import string    \n",
    "    def to_english(s):\n",
    "        # remove extra spaces\n",
    "        arr = re.sub('\\s', ' ', s)\n",
    "        arr = arr.split(' ')\n",
    "        retval = []\n",
    "        for word in arr:\n",
    "            #check word for alphanumeric characters by removing punctuations\n",
    "            if word.translate(string.punctuation).isalnum() : \n",
    "                retval.append(word.strip())\n",
    "        return ' '.join(retval)\n",
    "\n",
    "    \"\"\"\n",
    "    Removing non english words and punctuations\n",
    "    \"\"\"\n",
    "    non_english_and_punct_removed = []\n",
    "    for line in headers_removed_corpus:\n",
    "        clean_line = []\n",
    "        #print(line)\n",
    "        #remove punctuations using regex\n",
    "        line = re.sub(\"[:,-]\", ' ', line) \n",
    "        line = re.sub(\"[!\\\"#$%&\\'()\\*\\+,\\-\\./:;<=>?@\\[\\\\\\]^_`{|}~]\", ' ', line) #,'!\"#$%&()*,-.:;<=>?@^_`{|}~'\n",
    "        #print(line)\n",
    "        words = line.split(' ')\n",
    "        for word in words:\n",
    "            clean_line.append(to_english(word).strip())\n",
    "        clean_line = (' '.join(clean_line)).strip()\n",
    "        # remove extra spaces\n",
    "        clean_line = re.sub('\\s +', ' ', clean_line)\n",
    "        #filter empty strings\n",
    "        if clean_line != '':\n",
    "            non_english_and_punct_removed.append(clean_line)\n",
    "    #pprint(non_english_and_punct_removed)\n",
    "\n",
    "    \"\"\"\n",
    "    remove stopwords\n",
    "    \"\"\"\n",
    "    stop_words_removed = []\n",
    "    \n",
    "    for line in non_english_and_punct_removed:\n",
    "        words = line.split(' ')\n",
    "        new_line = []\n",
    "        for word in words:\n",
    "            # remove numbers\n",
    "            word = re.sub(\"[0-9]+\", '', word)\n",
    "            # remove extra spaces\n",
    "            word = re.sub(\"\\s\", ' ', word)\n",
    "            #convert word to lawer case\n",
    "            word = word.strip().lower()\n",
    "            if word == '' :\n",
    "                continue\n",
    "            if word not in stop_words:\n",
    "                new_line.append(word)\n",
    "        new_line = ' '.join(new_line)\n",
    "        if new_line != '' :\n",
    "            stop_words_removed.append(new_line)\n",
    "    #pprint(stop_words_removed)\n",
    "    \n",
    "    final_data = '.'.join(stop_words_removed)\n",
    "    return final_data\n",
    "\n",
    "\"\"\"\n",
    "dummy function to find paths of all files in data files folder\n",
    "\"\"\"\n",
    "def find_all_paths():\n",
    "    document_paths = []\n",
    "    from pprint import pprint\n",
    "    import os\n",
    "    walk = os.walk('.\\\\20_newsgroups', topdown = False)\n",
    "    for root, dirs, files in walk :\n",
    "        for file in files:\n",
    "            doc = {}\n",
    "            doc['root'] = root # path to folder containing the data files\n",
    "            doc['file'] = file # file name\n",
    "            document_paths.append(doc)\n",
    "    return document_paths\n",
    "#pprint(document_paths[0:100])\n",
    "\n",
    "\"\"\"\n",
    "dummy function to preprocess all the data files and create new clean data files\n",
    "\n",
    "\"\"\"\n",
    "def clean_all_docs(document_paths, remove_headers = True) :\n",
    "    corpus = \"\"\n",
    "    i = 0\n",
    "    import time\n",
    "    st = time.time()\n",
    "    new_paths = []\n",
    "    for doc_path in document_paths:\n",
    "        #if i == 2 :\n",
    "        #    break\n",
    "        path = doc_path['root'] + \"\\\\\" + doc_path['file']\n",
    "        with open(path) as doc :\n",
    "            data = doc.readlines()\n",
    "            i += 1\n",
    "            # preprocess data of currnt file\n",
    "            clean_corpus = preprocess_corpus(data, headers_list, remove_headers)\n",
    "            \n",
    "            clean_data_file_root = doc_path['root'].replace('.\\\\','.\\\\clean_data\\\\')\n",
    "            \n",
    "            # create directory if does not already exist\n",
    "            os.makedirs(clean_data_file_root, exist_ok = True)\n",
    "            # actual path of clean data file \n",
    "            clean_data_file_path = clean_data_file_root + \"\\\\\" +doc_path['file'] + '.txt'\n",
    "            # write to clean data file\n",
    "            with open(clean_data_file_path, 'wb') as file_clean_data :\n",
    "                file_clean_data.write(bytes(clean_corpus,'utf8'))\n",
    "                file_clean_data.close()\n",
    "            doc.close()\n",
    "            \n",
    "            #verbose\n",
    "            if i % 1000 == 0 :\n",
    "                print( i, \"Files Processed in\", round(time.time() - st, 3), \"sec\")\n",
    "                st = time.time()\n",
    "\n",
    "\n",
    "document_paths = find_all_paths()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Option to remove headers since for this dataset, removing headers actually decreases accuracy of classifier\n",
    "\n",
    "\"\"\"\n",
    "clean_all_docs(document_paths, remove_headers = True)\n",
    "#pprint(new_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos',\n",
      " 'talk.politics.mideast',\n",
      " 'rec.sport.hockey',\n",
      " 'comp.windows.x',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'rec.motorcycles',\n",
      " 'sci.med',\n",
      " 'sci.crypt',\n",
      " 'talk.politics.guns',\n",
      " 'misc.forsale',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'rec.sport.baseball',\n",
      " 'alt.atheism',\n",
      " 'sci.electronics',\n",
      " 'soc.religion.christian',\n",
      " 'sci.space']\n",
      "{0: 'rec.autos',\n",
      " 1: 'talk.politics.mideast',\n",
      " 2: 'rec.sport.hockey',\n",
      " 3: 'comp.windows.x',\n",
      " 4: 'comp.graphics',\n",
      " 5: 'comp.os.ms-windows.misc',\n",
      " 6: 'rec.motorcycles',\n",
      " 7: 'sci.med',\n",
      " 8: 'sci.crypt',\n",
      " 9: 'talk.politics.guns',\n",
      " 10: 'misc.forsale',\n",
      " 11: 'talk.politics.misc',\n",
      " 12: 'talk.religion.misc',\n",
      " 13: 'comp.sys.ibm.pc.hardware',\n",
      " 14: 'comp.sys.mac.hardware',\n",
      " 15: 'rec.sport.baseball',\n",
      " 16: 'alt.atheism',\n",
      " 17: 'sci.electronics',\n",
      " 18: 'soc.religion.christian',\n",
      " 19: 'sci.space'}\n",
      "0    .\\clean_data\\20_newsgroups\\alt.atheism\\49960.txt\n",
      "1    .\\clean_data\\20_newsgroups\\alt.atheism\\51060.txt\n",
      "2    .\\clean_data\\20_newsgroups\\alt.atheism\\51119.txt\n",
      "3    .\\clean_data\\20_newsgroups\\alt.atheism\\51120.txt\n",
      "4    .\\clean_data\\20_newsgroups\\alt.atheism\\51121.txt\n",
      "Name: path, dtype: object\n",
      "0    16\n",
      "1    16\n",
      "2    16\n",
      "3    16\n",
      "4    16\n",
      "Name: class, dtype: int64\n",
      "(14997,)\n",
      "(5000,)\n",
      "(14997,)\n",
      "(5000,)\n",
      "1000 Files Processed in 2.036 sec\n",
      "2000 Files Processed in 2.011 sec\n",
      "3000 Files Processed in 1.938 sec\n",
      "4000 Files Processed in 2.004 sec\n",
      "5000 Files Processed in 2.216 sec\n",
      "6000 Files Processed in 2.07 sec\n",
      "7000 Files Processed in 2.223 sec\n",
      "8000 Files Processed in 2.239 sec\n",
      "9000 Files Processed in 2.002 sec\n",
      "10000 Files Processed in 2.002 sec\n",
      "11000 Files Processed in 2.058 sec\n",
      "12000 Files Processed in 2.053 sec\n",
      "13000 Files Processed in 2.015 sec\n",
      "14000 Files Processed in 2.058 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from nltk.corpus import words as nltk_words\n",
    "\n",
    "#english dictionary contains english words from NLTK, dictionary for fast access\n",
    "english_dictionary = {word : True for word in list(set(nltk_words.words()))}\n",
    "\n",
    "\"\"\"\n",
    "check if input word is an english word \n",
    "\"\"\"\n",
    "\n",
    "def is_in_english(word):\n",
    "    try:\n",
    "        return english_dictionary[word]\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\"\"\"\n",
    "Build Dictionary of Words\n",
    "\"\"\"\n",
    "\n",
    "# document paths stores paths to all clean data files \n",
    "document_paths = []\n",
    "\n",
    "# pretty print module for better output format\n",
    "from pprint import pprint\n",
    "import os\n",
    "# find all file paths in clean data folder\n",
    "walk = os.walk('.\\\\clean_data\\\\20_newsgroups', topdown = False)\n",
    "for root, dirs, files in walk :\n",
    "    for file in files:\n",
    "        doc = {}\n",
    "        dir_ = root.split('\\\\')[-1]\n",
    "        doc['path'] = root+\"\\\\\"+file\n",
    "        doc['target'] = dir_\n",
    "        document_paths.append(doc)\n",
    "#pprint(document_paths[0:10000:100])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(document_paths)\n",
    "#df.head()\n",
    "\n",
    "# targets is list of all class names\n",
    "targets = list(set(df['target'].values))\n",
    "pprint(targets)\n",
    "\n",
    "# class_dict contains indices mapped to actual class names\n",
    "class_dict = { i : targets[i] for i in range(len(targets))}\n",
    "pprint(class_dict)\n",
    "\n",
    "# class is numeric representation for class name\n",
    "# categry is the actual class name\n",
    "class_df = pd.DataFrame()\n",
    "class_df['class'] = class_dict\n",
    "class_df['category'] = [class_dict[key] for key in class_dict]\n",
    "class_df.to_csv(\"class_dict.csv\", index = False)\n",
    "\n",
    "df['class'] = [targets.index(target) for target in df['target'].values]\n",
    "\n",
    "df.head()\n",
    "\n",
    "X = df['path']\n",
    "Y = df['class']\n",
    "print(X[0:5])\n",
    "print(Y[0:5])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Split the final data files into training and testing datasets(actually containing the file paths)\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 0) \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['path'] = X_train\n",
    "train_df['class'] = Y_train\n",
    "\n",
    "#save file storing the paths to train files\n",
    "train_df.to_csv(\"train_files.csv\", index = False)\n",
    "\n",
    "#save file storing the paths to test files\n",
    "test_df = pd.DataFrame()\n",
    "test_df['path'] = X_test\n",
    "test_df['class'] = Y_test\n",
    "test_df.to_csv(\"test_files.csv\", index = False)\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "test_df.head()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Build Master Dictionary that is used to decide the features \n",
    "\n",
    "master_dictionary is mapping of each english word with its count\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "master_dictionary = {}\n",
    "\n",
    "i = 0\n",
    "import time\n",
    "st = time.time()\n",
    "new_paths = []\n",
    "for doc_path in X_train:\n",
    "    #if i == 1 :\n",
    "    #    break\n",
    "    path = doc_path\n",
    "    with open(path) as doc :\n",
    "        num_tokens_in_doc = 0\n",
    "        data = ''.join(doc.readlines()).split('.')\n",
    "        for line in data:\n",
    "            for word in line.split():\n",
    "                # non english words can't be used as features for this dataset\n",
    "                if not is_in_english(word):\n",
    "                    continue\n",
    "                #print(word)\n",
    "                # using try/catch approach for very fast search\n",
    "                try:\n",
    "                    master_dictionary[word] += 1\n",
    "                except KeyError:\n",
    "                    master_dictionary[word] = 1\n",
    "                    continue\n",
    "        doc.close()\n",
    "        i += 1\n",
    "        # verbose\n",
    "        if i % 1000 == 0 :\n",
    "            print( i, \"Files Processed in\", round((time.time() - st), 3) , \"sec\")\n",
    "            st = time.time()\n",
    "#print(master_dictionary)\n",
    "\n",
    "import numpy as np\n",
    "keys = list(master_dictionary)\n",
    "freq = np.array([ master_dictionary[key] for key in keys])\n",
    "\n",
    "words_df = pd.DataFrame()\n",
    "words_df['word'] = keys\n",
    "words_df['frequency'] = freq\n",
    "\n",
    "# all words csv contains master dictionary\n",
    "words_df.to_csv('all_words.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9923\n",
      "6376\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDABJ6pQBIEmdMgAkqVMGgCR16v8BSTYHkS7D6iYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24235fc9dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of common words :  0\n",
      "No. of rare words :  14868\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2423641dd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6358\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x242365326a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load master dictionary\n",
    "words_df = pd.DataFrame(pd.read_csv(\"all_words.csv\"))\n",
    "\n",
    "words_df.head()\n",
    "\n",
    "all_words = words_df['word']\n",
    "freq = words_df['frequency']\n",
    "\n",
    "# rebuild master dictionary object\n",
    "master_dictionary = {all_words[i] : freq[i] for i in range(len(freq))}\n",
    "\n",
    "print(max(freq))\n",
    "\n",
    "# feature frequency range to be set here\n",
    "max_allowed_freq = max(freq)\n",
    "min_allowed_freq = 15\n",
    "\n",
    "# plot histogram just for fun\n",
    "filtered_freq = [f for f in freq if (f <= max_allowed_freq and f >= min_allowed_freq)]\n",
    "print(len(filtered_freq))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(filtered_freq, 50)\n",
    "plt.show()\n",
    "\n",
    "# ignore\n",
    "too_common_words = [word for word in master_dictionary if master_dictionary[word] > max_allowed_freq]\n",
    "print(\"No. of common words : \", len(too_common_words))\n",
    "\n",
    "\n",
    "# ignore\n",
    "too_rare_words = [word for word in master_dictionary if master_dictionary[word] < min_allowed_freq]\n",
    "print(\"No. of rare words : \", len(too_rare_words))\n",
    "#print(too_rare_words)\n",
    "\n",
    "\"\"\"\n",
    "Visualize the range of frequencies of selected words \n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "Y_freq = freq[freq >= min_allowed_freq]\n",
    "Y_freq = Y_freq[Y_freq <= max_allowed_freq]\n",
    "plt.plot(np.sort(Y_freq))\n",
    "plt.show()\n",
    "\n",
    "selected_words = [word for word in all_words \n",
    "                  if master_dictionary[word] >= min_allowed_freq \n",
    "                  and master_dictionary[word] <= max_allowed_freq \n",
    "                  and type(word) == str \n",
    "                  and len(word) > 1]\n",
    "\n",
    "#for word in keys:\n",
    "#    if freq\n",
    "print(len(selected_words))\n",
    "#print(selected_words)\n",
    "\n",
    "selected_words_freq = [master_dictionary[word] for word in selected_words]\n",
    "plt.plot(np.sort(selected_words_freq))\n",
    "plt.show()\n",
    "\n",
    "selected_words_df = pd.DataFrame()\n",
    "selected_words_df['word'] = selected_words\n",
    "selected_words_df['frequency'] = selected_words_freq\n",
    "\n",
    "\"\"\"\n",
    "selected words are words chosen as features from the master dictionary\n",
    "\"\"\"\n",
    "selected_words_df.to_csv('selected_words.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Files Processed in 5.877 sec\n",
      "2000 Files Processed in 5.651 sec\n",
      "3000 Files Processed in 5.743 sec\n",
      "4000 Files Processed in 5.824 sec\n",
      "5000 Files Processed in 5.942 sec\n",
      "6000 Files Processed in 6.994 sec\n",
      "7000 Files Processed in 6.603 sec\n",
      "8000 Files Processed in 6.596 sec\n",
      "9000 Files Processed in 7.607 sec\n",
      "10000 Files Processed in 12.992 sec\n",
      "11000 Files Processed in 8.239 sec\n",
      "12000 Files Processed in 9.222 sec\n",
      "13000 Files Processed in 8.497 sec\n",
      "14000 Files Processed in 11.277 sec\n",
      "(14997, 6358)\n",
      "(14997,)\n",
      "1000 Files Processed in 5.942 sec\n",
      "2000 Files Processed in 5.716 sec\n",
      "3000 Files Processed in 5.662 sec\n",
      "4000 Files Processed in 5.897 sec\n",
      "5000 Files Processed in 5.944 sec\n",
      "(5000, 6358)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Build Dataset of word counts as features\n",
    "\"\"\"\n",
    "def prepare_dataset(selected_words_file_path, data_files_path):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # load class dict\n",
    "    class_dict_df = pd.DataFrame(pd.read_csv(\"class_dict.csv\"))\n",
    "    #class_dict_df.head()\n",
    "\n",
    "    # load selected words dictionary\n",
    "    #selected_words_df = pd.DataFrame(pd.read_csv('all_words.csv'))\n",
    "    selected_words_df = pd.DataFrame(pd.read_csv(selected_words_file_path))\n",
    "    #selected_words_df.head()\n",
    "\n",
    "    selected_words = selected_words_df['word'].values\n",
    "    \n",
    "    selected_words_dictionary = {word : True for word in selected_words}\n",
    "    \n",
    "    \n",
    "    #number of selected words\n",
    "    num_selected_words = len(selected_words)\n",
    "\n",
    "    #load file containing document paths\n",
    "    data_file_paths_df = pd.DataFrame(pd.read_csv(data_files_path))\n",
    "    #data_file_paths_df.head()\n",
    "\n",
    "    data_file_paths = data_file_paths_df['path'].values\n",
    "    \n",
    "    #directly used as target Y\n",
    "    data_file_class = data_file_paths_df['class'].values\n",
    "\n",
    "    import time\n",
    "    matrix = []\n",
    "    st = time.time()\n",
    "    for i in range(len(data_file_paths)) :\n",
    "        #if i == 2000:\n",
    "        #    break\n",
    "    \n",
    "        if (i+1) % 1000 == 0 :\n",
    "            et = time.time()\n",
    "            print( i+1, \"Files Processed in\", round((time.time() - st), 3) , \"sec\")\n",
    "            st = time.time()    \n",
    "        path = data_file_paths[i]\n",
    "        target = data_file_class[i]\n",
    "        with open(path) as file :\n",
    "            X = []\n",
    "            count = {}\n",
    "            data = file.readlines()\n",
    "            words_in_file = (''.join(data)).split(' ')\n",
    "            for word in words_in_file:\n",
    "                try:\n",
    "                    if selected_words_dictionary[word]:\n",
    "                        # try catch approach for fast search\n",
    "                        try:\n",
    "                            count[word] += 1\n",
    "                        except:\n",
    "                            count[word] = 1\n",
    "                except:\n",
    "                    continue\n",
    "            for word in selected_words:\n",
    "                # try catch approach for fast search\n",
    "                try :\n",
    "                    X = count[word] \n",
    "                except :\n",
    "                    count[word] = 0 \n",
    "            #print(X)\n",
    "            X = [count[word] for word in selected_words if word in count] \n",
    "            #print(len(X))\n",
    "            matrix.append(X)\n",
    "            #X_train_df.iloc[i, :] = \n",
    "            file.close()\n",
    "    #print(len(matrix))\n",
    "    #print(len(matrix[0]))\n",
    "\n",
    "    X_df = pd.DataFrame(matrix, columns = selected_words)\n",
    "\n",
    "    #X_df.describe()\n",
    "\n",
    "    import copy\n",
    "    X = X_df.values\n",
    "    Y = copy.deepcopy(data_file_class)\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    dataset_df = pd.DataFrame(X_df)\n",
    "    dataset_df['target'] = Y\n",
    "    return dataset_df\n",
    "\n",
    "train_files_path = \"train_files.csv\"\n",
    "test_files_path = \"test_files.csv\"\n",
    "\n",
    "selected_words_file_path = 'selected_words.csv'\n",
    "\n",
    "train_dataset_df = prepare_dataset(selected_words_file_path, train_files_path)\n",
    "train_dataset_df.to_csv(\"20_newsgroups_dataset_min_freq_15.csv\", index = False)\n",
    "\n",
    "test_dataset_df = prepare_dataset(selected_words_file_path, test_files_path)\n",
    "test_dataset_df.to_csv(\"20_newsgroups_test_dataset_min_freq_15.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Time for Training Set: 16715 millisecond\n",
      "Loading Time for Test Set: 5839 millisecond\n",
      "Training Time: 11131 millisecond\n",
      "Prediction Time for Training Set: 10367 millisecond\n",
      "Confusion Matrix for Training Dataset\n",
      "array([[633,   4,   1,   1,   1,   2,   7,   2,   1,   4,  12,   4,   3,\n",
      "          4,   4,   4,   1,   8,   2,   7],\n",
      "       [  1, 599,   1,   1,   1,   1,   3,   4,   3,   4,   2,  30,  11,\n",
      "          2,   2,   3,  11,   3,   5,   2],\n",
      "       [  7,   8, 711,   6,   7,  14,   5,  10,   6,   7,  15,   5,   6,\n",
      "          4,   6,  31,   6,   5,   4,   2],\n",
      "       [  2,   3,   1, 581,  45,  26,   1,   4,   6,   6,   5,   0,   2,\n",
      "         13,   7,   0,   4,  10,   2,   3],\n",
      "       [  2,   1,   2,  46, 527,  20,   3,  11,  11,   0,   5,   2,   5,\n",
      "         17,  11,   6,   1,  10,   1,   8],\n",
      "       [  0,   3,   2,  35,  45, 580,   1,   4,  12,   2,  15,   0,   4,\n",
      "         14,  21,   2,   0,  14,   2,   6],\n",
      "       [ 21,   3,   3,   6,   5,   7, 644,  11,   2,   2,   8,   3,   3,\n",
      "          4,   8,  11,   6,   2,   3,   7],\n",
      "       [  2,   4,   0,   4,   5,   3,   0, 635,   2,   1,   2,   3,   4,\n",
      "          2,   5,   2,   2,   5,   5,   5],\n",
      "       [  3,   5,   1,   7,   4,   3,   1,   6, 683,  11,   3,   2,   6,\n",
      "          6,   4,   1,   3,  11,   4,   3],\n",
      "       [  6,  10,   2,   1,   4,   4,   4,   7,   9, 654,   2,  34,  32,\n",
      "          2,   1,   6,  11,   0,   4,   2],\n",
      "       [ 10,   5,   5,  21,  20,  16,  10,   8,   5,   5, 576,   3,   3,\n",
      "         23,  19,   6,   2,  11,   2,  10],\n",
      "       [  3,  23,   2,   1,   2,   2,   2,   3,   3,  19,   5, 567,  46,\n",
      "          0,   1,   1,  10,   1,   7,   9],\n",
      "       [  2,   8,   0,   4,   1,   3,   2,   7,   3,  14,   2,  61, 501,\n",
      "          1,   3,   2,  53,   2,  12,   4],\n",
      "       [  7,   2,   1,  11,  31,  33,  10,   3,   3,   0,  41,   1,   4,\n",
      "        606,  31,   7,   2,  22,   0,   2],\n",
      "       [  4,   5,   3,  15,  16,  20,   7,   6,   4,   1,  27,   3,   2,\n",
      "         36, 625,   4,   3,  22,   1,   5],\n",
      "       [  7,   8,  27,   0,  11,   3,   7,   2,   5,   4,   7,   5,   4,\n",
      "          7,   4, 660,   5,   7,   1,   3],\n",
      "       [  2,  18,   3,   2,   5,   1,   0,   5,   2,   8,   3,   6, 109,\n",
      "          1,   0,   1, 628,   2,  17,   5],\n",
      "       [ 12,   2,   1,   9,   7,   4,   5,   6,   3,   3,   4,   4,   2,\n",
      "         14,   8,   2,   1, 618,   1,   4],\n",
      "       [  1,   5,   1,   3,   2,   3,   2,   6,   4,   2,   3,   4,  12,\n",
      "          1,   0,   2,  14,   0, 670,   1],\n",
      "       [  6,   3,   2,   6,   8,   6,   2,   4,   0,   4,   2,   4,   5,\n",
      "          3,   4,   1,   4,   3,   2, 666]], dtype=int64)\n",
      "classification Report for Training Dataset\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.90      0.88       705\n",
      "          1       0.83      0.87      0.85       689\n",
      "          2       0.92      0.82      0.87       865\n",
      "          3       0.76      0.81      0.78       721\n",
      "          4       0.71      0.76      0.73       689\n",
      "          5       0.77      0.76      0.77       762\n",
      "          6       0.90      0.85      0.87       759\n",
      "          7       0.85      0.92      0.89       691\n",
      "          8       0.89      0.89      0.89       767\n",
      "          9       0.87      0.82      0.85       795\n",
      "         10       0.78      0.76      0.77       760\n",
      "         11       0.77      0.80      0.78       707\n",
      "         12       0.66      0.73      0.69       685\n",
      "         13       0.80      0.74      0.77       817\n",
      "         14       0.82      0.77      0.79       809\n",
      "         15       0.88      0.85      0.86       777\n",
      "         16       0.82      0.77      0.79       818\n",
      "         17       0.82      0.87      0.84       710\n",
      "         18       0.90      0.91      0.90       736\n",
      "         19       0.88      0.91      0.89       735\n",
      "\n",
      "avg / total       0.83      0.82      0.82     14997\n",
      "\n",
      "Prediction Time for Testing Set: 3502 millisecond\n",
      "3.5023443698883057\n",
      "Confusion Matrix for Test Dataset\n",
      "array([[187,   0,   3,   4,   1,   0,  22,   3,   0,   2,  17,   4,   3,\n",
      "          1,   1,   1,   2,   8,   1,   6],\n",
      "       [  0, 199,   2,   1,   0,   1,   2,   7,   1,  10,   5,  26,   6,\n",
      "          1,   0,   2,   8,   0,   4,   3],\n",
      "       [  2,   1, 184,   6,   2,   5,   2,   0,   1,   5,   8,   3,   1,\n",
      "          2,   3,  23,   1,   2,   2,   2],\n",
      "       [  2,   1,   0, 136,  13,  17,   0,   1,   2,   0,   0,   0,   1,\n",
      "          3,   3,   2,   2,   2,   2,   3],\n",
      "       [  1,   0,   1,  27, 151,  17,   0,   4,   1,   0,  14,   2,   1,\n",
      "          9,   3,   2,   1,  14,   1,   5],\n",
      "       [  2,   2,   2,  16,  25, 128,   2,   2,   2,   0,   7,   1,   3,\n",
      "         22,  15,   1,   1,   5,   1,   3],\n",
      "       [ 17,   3,   1,   0,   6,   6, 205,   5,   3,   0,  11,   2,   0,\n",
      "          3,   1,   7,   4,   7,   0,   4],\n",
      "       [  4,   2,   1,   4,   4,   2,   2, 194,   3,   1,   0,   4,   2,\n",
      "          0,   0,   3,   1,   1,   2,   4],\n",
      "       [  3,   5,   0,   8,   3,  10,   5,   3, 190,   6,   1,   4,   1,\n",
      "          4,   4,   0,   2,   5,   0,   0],\n",
      "       [  6,   4,   4,   0,   1,   0,   7,   4,   9, 165,   6,  23,  13,\n",
      "          0,   0,   3,   8,   0,   1,   3],\n",
      "       [  7,   6,   2,   9,   6,   8,   8,   5,   2,   3, 130,   2,   2,\n",
      "         11,  15,   5,   2,  11,   3,   1],\n",
      "       [  6,  30,   3,   0,   1,   2,   6,   9,  11,  38,   4, 142,  27,\n",
      "          0,   0,   5,  10,   1,   7,   5],\n",
      "       [  2,   7,   0,   1,   3,   0,   2,   4,   0,  10,   0,  29,  86,\n",
      "          1,   2,   0,  48,   1,  23,   4],\n",
      "       [  4,   2,   2,   6,  13,  25,   2,   1,   0,   0,  15,   4,   3,\n",
      "        141,  33,   1,   1,  15,   1,   0],\n",
      "       [  7,   3,   1,   7,   8,  18,   3,   2,   1,   1,  17,   2,   2,\n",
      "         28, 147,   0,   1,  16,   1,   2],\n",
      "       [  2,   2,  21,   2,   5,   0,   6,   2,   0,   2,   7,   0,   2,\n",
      "          1,   1, 187,   1,   4,   1,   6],\n",
      "       [  4,  10,   2,   2,   3,   1,   0,   3,   3,   5,   1,   3,  63,\n",
      "          0,   0,   1, 130,   1,  13,   3],\n",
      "       [ 13,   0,   1,   3,   1,   4,   8,   0,   1,   0,  13,   0,   0,\n",
      "         13,   7,   1,   1, 140,   2,   6],\n",
      "       [  0,   4,   1,   0,   0,   1,   0,   0,   1,   1,   0,   4,  18,\n",
      "          0,   0,   1,   6,   1, 184,   0],\n",
      "       [  0,   0,   0,   8,   7,   4,   2,   7,   2,   0,   5,   4,   2,\n",
      "          0,   1,   3,   3,  10,   3, 186]], dtype=int64)\n",
      "classification Report for Test Dataset\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.70      0.70       266\n",
      "          1       0.71      0.72      0.71       278\n",
      "          2       0.80      0.72      0.76       255\n",
      "          3       0.57      0.72      0.63       190\n",
      "          4       0.60      0.59      0.60       254\n",
      "          5       0.51      0.53      0.52       240\n",
      "          6       0.72      0.72      0.72       285\n",
      "          7       0.76      0.83      0.79       234\n",
      "          8       0.82      0.75      0.78       254\n",
      "          9       0.66      0.64      0.65       257\n",
      "         10       0.50      0.55      0.52       238\n",
      "         11       0.55      0.46      0.50       307\n",
      "         12       0.36      0.39      0.37       223\n",
      "         13       0.59      0.52      0.55       269\n",
      "         14       0.62      0.55      0.58       267\n",
      "         15       0.75      0.74      0.75       252\n",
      "         16       0.56      0.52      0.54       248\n",
      "         17       0.57      0.65      0.61       214\n",
      "         18       0.73      0.83      0.78       222\n",
      "         19       0.76      0.75      0.75       247\n",
      "\n",
      "avg / total       0.64      0.64      0.64      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load training dataset\n",
    "import time\n",
    "st = time.time()\n",
    "train_dataset_df = pd.read_csv(\"20_newsgroups_dataset_min_freq_15.csv\")\n",
    "et = time.time()\n",
    "\n",
    "print(\"Loading Time for Training Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "train_dataset_df.describe()\n",
    "\n",
    "X_train_df = train_dataset_df.drop('target', axis = 1) \n",
    "Y_train_df = train_dataset_df['target']\n",
    "\n",
    "# X_train_df.describe()\n",
    "\n",
    "# Y_train_df.describe()\n",
    "\n",
    "# load testing dataset\n",
    "import time\n",
    "st = time.time()\n",
    "test_dataset_df = pd.read_csv(\"20_newsgroups_test_dataset_min_freq_15.csv\")\n",
    "et = time.time()\n",
    "\n",
    "print(\"Loading Time for Test Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "test_dataset_df.describe()\n",
    "\n",
    "X_test_df = test_dataset_df.drop('target', axis = 1)\n",
    "Y_test_df = test_dataset_df['target']\n",
    "\n",
    "# X_test_df.describe()\n",
    "\n",
    "# Y_test_df.describe()\n",
    "\n",
    "words = X_train_df.columns\n",
    "#print(words)\n",
    "possible_classes = list(set(Y_train_df.values))\n",
    "#print(possible_classes)\n",
    "\n",
    "# predict class for single document\n",
    "def predict_single(X, model) :\n",
    "    \n",
    "    # initial max class probablity, and assigned class\n",
    "    max_class_prob = -np.inf\n",
    "    max_class = None\n",
    "    # all possible classes\n",
    "    possible_classes = model.keys()\n",
    "    # find probablities of this document belonging to each of the possible classes \n",
    "    for y in possible_classes :\n",
    "        # convert input array to numpy array in case it isn't\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # assign class prior\n",
    "        prob_X_equals_x_given_Y_equals_y = model[y]['class_prior']\n",
    "        # multinomial feature probability addition to get probability \n",
    "        # that this document belongs to current class\n",
    "        prob_X_equals_x_given_Y_equals_y += (X * model[y]['log_prob_sum']).sum()\n",
    "        \n",
    "        # if prob is max update max_class variables\n",
    "        if prob_X_equals_x_given_Y_equals_y > max_class_prob :\n",
    "            max_class_prob = prob_X_equals_x_given_Y_equals_y\n",
    "            max_class = y\n",
    "    return max_class\n",
    "\n",
    "\"\"\"\n",
    "Internal function\n",
    "generate_model takes as input a dictionary and \n",
    "a tuning parameter alpha that accounts for probability correction(similar to the inbuilt classifer)\n",
    "\"\"\"\n",
    "def generate_model(dictionary, alpha) :\n",
    "    model = {}\n",
    "    possible_classes = dictionary['possible_classes']\n",
    "    num_words = dictionary[\"vocabulary_size\"]\n",
    "    \n",
    "    range_words = range(num_words)\n",
    "    \n",
    "    for y in possible_classes :\n",
    "        #class_prior\n",
    "        model[y] = {}\n",
    "        prob_Y_equals_y = np.log(dictionary[y][\"class_count\"]/dictionary[\"total_data\"])\n",
    "        prob_X_equals_x_given_Y_equals_y = 0 \n",
    "        prob_X_equals_x_given_Y_equals_y += prob_Y_equals_y\n",
    "        total_words_in_class_y_docs = dictionary[y][\"total_words\"]        \n",
    "        count_f_class_y = np.array([dictionary[y][f] for f in range_words])\n",
    "        prob_f_class_y = (count_f_class_y+alpha)/(total_words_in_class_y_docs + alpha*num_words) \n",
    "        \n",
    "        model[y]['class_prior'] = prob_Y_equals_y\n",
    "        model[y]['log_prob_sum'] = np.log(prob_f_class_y)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def predict(X, model) :\n",
    "    #if not isinstance(X[0], list) :\n",
    "    #    return None #predict_single(X, model)\n",
    "    Y_pred = []\n",
    "    for x in X :\n",
    "        Y_pred.append(predict_single(x, model))\n",
    "    return Y_pred\n",
    "\n",
    "\"\"\"\n",
    "fit function takes as input the trainig dataset and alpha, the tuning parameter\n",
    "\"\"\"\n",
    "def fit(X, Y, alpha) :\n",
    "    # check if alpha is in range of [0, 1]\n",
    "    # if not set alpha to 1\n",
    "    if alpha > 1 or alpha < 0 :\n",
    "        print(\"Alpha parameter not in range [0,1]...\")\n",
    "        print(\"setting alpha = 1\")\n",
    "        alpha = 1\n",
    "    num_words = len(X[0])\n",
    "    possible_classes = list(set(Y))\n",
    "    \n",
    "    #dictionary with keys as possible classes, total number of training documents, vocabulary size\n",
    "    dictionary = {}\n",
    "    dictionary[\"total_data\"] = len(Y)\n",
    "    dictionary[\"vocabulary_size\"] = num_words\n",
    "    dictionary[\"possible_classes\"] = possible_classes\n",
    "    \n",
    "    # build internal dictionaries for each of the possible classes\n",
    "    # dict mapped to each class contains keys: \n",
    "    # total words and \n",
    "    # a class count that is the total number of training documents belonging to current class \n",
    "    \n",
    "    for y in possible_classes :\n",
    "        y_dict = {i : X[Y == y, i].sum() for i in range(num_words)}\n",
    "        y_dict['total_words'] = sum(y_dict.values())\n",
    "        y_dict['class_count'] = sum(Y == y)\n",
    "        #print(y_dict['total_count'])\n",
    "        dictionary[y] = y_dict\n",
    "    # call is made to generate model function that uses the \"dictionary\" and \n",
    "    # alpha \n",
    "    # to find effect on probabilities of a document belonging to a particular class\n",
    "    # separate function for simplicity\n",
    "    return generate_model(dictionary, alpha)\n",
    "\n",
    "# fit the model\n",
    "import time\n",
    "st = time.time()\n",
    "model = fit(X_train_df.values, Y_train_df.values, 0.01)\n",
    "et = time.time()\n",
    "\n",
    "print(\"Training Time:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "#print(model)\n",
    "\n",
    "\"\"\"\n",
    "Train Data Evaluation\n",
    "\"\"\"\n",
    "import time\n",
    "st = time.time()\n",
    "Y_train_pred = predict(X_train_df.values, model)\n",
    "et = time.time()\n",
    "\n",
    "print(\"Prediction Time for Training Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "#print(Y_train_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"Confusion Matrix for Training Dataset\")\n",
    "pprint(confusion_matrix(Y_train_pred, Y_train_df.values))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"classification Report for Training Dataset\")\n",
    "print(classification_report(Y_train_pred, Y_train_df.values))\n",
    "\n",
    "\"\"\"\n",
    "Test Data Evaluation\n",
    "\"\"\"\n",
    "import time\n",
    "st = time.time()\n",
    "Y_test_pred = predict(X_test_df.values, model)\n",
    "et = time.time()\n",
    "\n",
    "print(\"Prediction Time for Testing Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "training_time = (et-st)\n",
    "print(training_time)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"Confusion Matrix for Test Dataset\")\n",
    "pprint(confusion_matrix(Y_test_pred, Y_test_df.values))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"classification Report for Test Dataset\")\n",
    "print(classification_report(Y_test_pred, Y_test_df.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 939 millisecond\n",
      "Prediction Time for Training Set: 707 millisecond\n",
      "Confusion Matrix for Training Dataset\n",
      "array([[633,   4,   1,   1,   1,   2,   7,   2,   1,   4,  12,   4,   3,\n",
      "          4,   4,   4,   1,   8,   2,   7],\n",
      "       [  1, 599,   1,   1,   1,   1,   3,   4,   3,   4,   2,  30,  11,\n",
      "          2,   2,   3,  11,   3,   5,   2],\n",
      "       [  7,   8, 711,   6,   7,  14,   5,  10,   6,   7,  15,   5,   6,\n",
      "          4,   6,  31,   6,   5,   4,   2],\n",
      "       [  2,   3,   1, 581,  45,  26,   1,   4,   6,   6,   5,   0,   2,\n",
      "         13,   7,   0,   4,  10,   2,   3],\n",
      "       [  2,   1,   2,  46, 527,  20,   3,  11,  11,   0,   5,   2,   5,\n",
      "         17,  11,   6,   1,  10,   1,   8],\n",
      "       [  0,   3,   2,  35,  45, 580,   1,   4,  12,   2,  15,   0,   4,\n",
      "         14,  21,   2,   0,  14,   2,   6],\n",
      "       [ 21,   3,   3,   6,   5,   7, 644,  11,   2,   2,   8,   3,   3,\n",
      "          4,   8,  11,   6,   2,   3,   7],\n",
      "       [  2,   4,   0,   4,   5,   3,   0, 635,   2,   1,   2,   3,   4,\n",
      "          2,   5,   2,   2,   5,   5,   5],\n",
      "       [  3,   5,   1,   7,   4,   3,   1,   6, 683,  11,   3,   2,   6,\n",
      "          6,   4,   1,   3,  11,   4,   3],\n",
      "       [  6,  10,   2,   1,   4,   4,   4,   7,   9, 654,   2,  34,  32,\n",
      "          2,   1,   6,  11,   0,   4,   2],\n",
      "       [ 10,   5,   5,  21,  20,  16,  10,   8,   5,   5, 576,   3,   3,\n",
      "         23,  19,   6,   2,  11,   2,  10],\n",
      "       [  3,  23,   2,   1,   2,   2,   2,   3,   3,  19,   5, 567,  46,\n",
      "          0,   1,   1,  10,   1,   7,   9],\n",
      "       [  2,   8,   0,   4,   1,   3,   2,   7,   3,  14,   2,  61, 501,\n",
      "          1,   3,   2,  53,   2,  12,   4],\n",
      "       [  7,   2,   1,  11,  31,  33,  10,   3,   3,   0,  41,   1,   4,\n",
      "        606,  31,   7,   2,  22,   0,   2],\n",
      "       [  4,   5,   3,  15,  16,  20,   7,   6,   4,   1,  27,   3,   2,\n",
      "         36, 625,   4,   3,  22,   1,   5],\n",
      "       [  7,   8,  27,   0,  11,   3,   7,   2,   5,   4,   7,   5,   4,\n",
      "          7,   4, 660,   5,   7,   1,   3],\n",
      "       [  2,  18,   3,   2,   5,   1,   0,   5,   2,   8,   3,   6, 109,\n",
      "          1,   0,   1, 628,   2,  17,   5],\n",
      "       [ 12,   2,   1,   9,   7,   4,   5,   6,   3,   3,   4,   4,   2,\n",
      "         14,   8,   2,   1, 618,   1,   4],\n",
      "       [  1,   5,   1,   3,   2,   3,   2,   6,   4,   2,   3,   4,  12,\n",
      "          1,   0,   2,  14,   0, 670,   1],\n",
      "       [  6,   3,   2,   6,   8,   6,   2,   4,   0,   4,   2,   4,   5,\n",
      "          3,   4,   1,   4,   3,   2, 666]], dtype=int64)\n",
      "classification Report for Training Dataset\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.90      0.88       705\n",
      "          1       0.83      0.87      0.85       689\n",
      "          2       0.92      0.82      0.87       865\n",
      "          3       0.76      0.81      0.78       721\n",
      "          4       0.71      0.76      0.73       689\n",
      "          5       0.77      0.76      0.77       762\n",
      "          6       0.90      0.85      0.87       759\n",
      "          7       0.85      0.92      0.89       691\n",
      "          8       0.89      0.89      0.89       767\n",
      "          9       0.87      0.82      0.85       795\n",
      "         10       0.78      0.76      0.77       760\n",
      "         11       0.77      0.80      0.78       707\n",
      "         12       0.66      0.73      0.69       685\n",
      "         13       0.80      0.74      0.77       817\n",
      "         14       0.82      0.77      0.79       809\n",
      "         15       0.88      0.85      0.86       777\n",
      "         16       0.82      0.77      0.79       818\n",
      "         17       0.82      0.87      0.84       710\n",
      "         18       0.90      0.91      0.90       736\n",
      "         19       0.88      0.91      0.89       735\n",
      "\n",
      "avg / total       0.83      0.82      0.82     14997\n",
      "\n",
      "Prediction Time for Test Set: 271 millisecond\n",
      "Confusion Matrix for Test Dataset\n",
      "array([[187,   0,   3,   4,   1,   0,  22,   3,   0,   2,  17,   4,   3,\n",
      "          1,   1,   1,   2,   8,   1,   6],\n",
      "       [  0, 199,   2,   1,   0,   1,   2,   7,   1,  10,   5,  26,   6,\n",
      "          1,   0,   2,   8,   0,   4,   3],\n",
      "       [  2,   1, 184,   6,   2,   5,   2,   0,   1,   5,   8,   3,   1,\n",
      "          2,   3,  23,   1,   2,   2,   2],\n",
      "       [  2,   1,   0, 136,  13,  17,   0,   1,   2,   0,   0,   0,   1,\n",
      "          3,   3,   2,   2,   2,   2,   3],\n",
      "       [  1,   0,   1,  27, 151,  17,   0,   4,   1,   0,  14,   2,   1,\n",
      "          9,   3,   2,   1,  14,   1,   5],\n",
      "       [  2,   2,   2,  16,  25, 128,   2,   2,   2,   0,   7,   1,   3,\n",
      "         22,  15,   1,   1,   5,   1,   3],\n",
      "       [ 17,   3,   1,   0,   6,   6, 205,   5,   3,   0,  11,   2,   0,\n",
      "          3,   1,   7,   4,   7,   0,   4],\n",
      "       [  4,   2,   1,   4,   4,   2,   2, 194,   3,   1,   0,   4,   2,\n",
      "          0,   0,   3,   1,   1,   2,   4],\n",
      "       [  3,   5,   0,   8,   3,  10,   5,   3, 190,   6,   1,   4,   1,\n",
      "          4,   4,   0,   2,   5,   0,   0],\n",
      "       [  6,   4,   4,   0,   1,   0,   7,   4,   9, 165,   6,  23,  13,\n",
      "          0,   0,   3,   8,   0,   1,   3],\n",
      "       [  7,   6,   2,   9,   6,   8,   8,   5,   2,   3, 130,   2,   2,\n",
      "         11,  15,   5,   2,  11,   3,   1],\n",
      "       [  6,  30,   3,   0,   1,   2,   6,   9,  11,  38,   4, 142,  27,\n",
      "          0,   0,   5,  10,   1,   7,   5],\n",
      "       [  2,   7,   0,   1,   3,   0,   2,   4,   0,  10,   0,  29,  86,\n",
      "          1,   2,   0,  48,   1,  23,   4],\n",
      "       [  4,   2,   2,   6,  13,  25,   2,   1,   0,   0,  15,   4,   3,\n",
      "        141,  33,   1,   1,  15,   1,   0],\n",
      "       [  7,   3,   1,   7,   8,  18,   3,   2,   1,   1,  17,   2,   2,\n",
      "         28, 147,   0,   1,  16,   1,   2],\n",
      "       [  2,   2,  21,   2,   5,   0,   6,   2,   0,   2,   7,   0,   2,\n",
      "          1,   1, 187,   1,   4,   1,   6],\n",
      "       [  4,  10,   2,   2,   3,   1,   0,   3,   3,   5,   1,   3,  63,\n",
      "          0,   0,   1, 130,   1,  13,   3],\n",
      "       [ 13,   0,   1,   3,   1,   4,   8,   0,   1,   0,  13,   0,   0,\n",
      "         13,   7,   1,   1, 140,   2,   6],\n",
      "       [  0,   4,   1,   0,   0,   1,   0,   0,   1,   1,   0,   4,  18,\n",
      "          0,   0,   1,   6,   1, 184,   0],\n",
      "       [  0,   0,   0,   8,   7,   4,   2,   7,   2,   0,   5,   4,   2,\n",
      "          0,   1,   3,   3,  10,   3, 186]], dtype=int64)\n",
      "classification Report for Test Dataset\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.70      0.70       266\n",
      "          1       0.71      0.72      0.71       278\n",
      "          2       0.80      0.72      0.76       255\n",
      "          3       0.57      0.72      0.63       190\n",
      "          4       0.60      0.59      0.60       254\n",
      "          5       0.51      0.53      0.52       240\n",
      "          6       0.72      0.72      0.72       285\n",
      "          7       0.76      0.83      0.79       234\n",
      "          8       0.82      0.75      0.78       254\n",
      "          9       0.66      0.64      0.65       257\n",
      "         10       0.50      0.55      0.52       238\n",
      "         11       0.55      0.46      0.50       307\n",
      "         12       0.36      0.39      0.37       223\n",
      "         13       0.59      0.52      0.55       269\n",
      "         14       0.62      0.55      0.58       267\n",
      "         15       0.75      0.74      0.75       252\n",
      "         16       0.56      0.52      0.54       248\n",
      "         17       0.57      0.65      0.61       214\n",
      "         18       0.73      0.83      0.78       222\n",
      "         19       0.76      0.75      0.75       247\n",
      "\n",
      "avg / total       0.64      0.64      0.64      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "sklearn_model = MultinomialNB(alpha = 0.01)\n",
    "sklearn_model.fit(X_train_df.values, Y_train_df.values)\n",
    "et = time.time()\n",
    "print(\"Training Time:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "Y_train_pred = sklearn_model.predict(X_train_df.values)\n",
    "et = time.time()\n",
    "print(\"Prediction Time for Training Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "print(\"Confusion Matrix for Training Dataset\")\n",
    "pprint(confusion_matrix(Y_train_pred, Y_train_df.values))\n",
    "\n",
    "print(\"classification Report for Training Dataset\")\n",
    "print(classification_report(Y_train_pred, Y_train_df.values))\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "Y_test_pred = sklearn_model.predict(X_test_df.values)\n",
    "et = time.time()\n",
    "print(\"Prediction Time for Test Set:\", round((et-st)*1000), \"millisecond\")\n",
    "\n",
    "print(\"Confusion Matrix for Test Dataset\")\n",
    "pprint(confusion_matrix(Y_test_pred, Y_test_df.values))\n",
    "\n",
    "print(\"classification Report for Test Dataset\")\n",
    "print(classification_report(Y_test_pred, Y_test_df.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of results obtained from own implementation and the inbuilt classifier in sklearn \n",
    "\n",
    "(referring to inbuilt classifier as MultinomialNB)\n",
    "\n",
    "1. With the same dataset and alpha parameter, MultinomialNB and my own implementation give exactly the same results.\n",
    "2. Tuning parameter alpha should be kept low for better accuracy.\n",
    "2. The speed of MultinomialNB is around 10 times, since its implemented with cython.\n",
    "3. Without removing headers, I got an F1 Score of 0.89(but using all words as features), MultinomialNB performed likewise.\n",
    "4. Increasing the number of features boosts the score of this claasifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
