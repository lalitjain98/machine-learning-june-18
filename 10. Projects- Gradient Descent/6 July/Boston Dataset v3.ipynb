{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent - Boston Dataset\n",
    "Boston dataset is one of the datasets available in sklearn.\n",
    "You are given a Training dataset csv file with X train and Y train data. As studied in lecture, your task is to come up with Gradient Descent algorithm and thus predictions for the test dataset given.\n",
    "Your task is to:\n",
    "    1. Code Gradient Descent for N features and come with predictions.\n",
    "    2. Try and test with various combinations of learning rates and number of iterations.\n",
    "    3. Try using Feature Scaling, and see if it helps you in getting better results. \n",
    "Read Instructions carefully -\n",
    "    1. Use Gradient Descent as a training algorithm and submit results predicted.\n",
    "    2. Files are in csv format, you can use genfromtxt function in numpy to load data from csv file. Similarly you can use savetxt function to save data into a file.\n",
    "    3. Submit a csv file with only predictions for X test data. File name should not have spaces. File should not have any headers and should only have one column i.e. predictions. Also predictions shouldn't be in exponential form. \n",
    "    4. Your score is based on coefficient of determination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
      "count  379.000000  379.000000  379.000000  379.000000  379.000000  379.000000   \n",
      "mean     0.019628    0.002455    0.036170    0.028955    0.028775    0.032202   \n",
      "std      1.067490    1.000813    1.017497    1.048995    0.999656    1.001174   \n",
      "min     -0.417713   -0.487722   -1.516987   -0.272599   -1.465882   -3.880249   \n",
      "25%     -0.408171   -0.487722   -0.867691   -0.272599   -0.878475   -0.571480   \n",
      "50%     -0.383729   -0.487722   -0.180458   -0.272599   -0.144217   -0.103479   \n",
      "75%      0.055208    0.156071    1.015999   -0.272599    0.628913    0.529069   \n",
      "max      9.941735    3.804234    2.422565    3.668398    2.732346    3.555044   \n",
      "\n",
      "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
      "count  379.000000  379.000000  379.000000  379.000000  379.000000  379.000000   \n",
      "mean     0.038395   -0.001288    0.043307    0.043786    0.019218   -0.015785   \n",
      "std      0.985209    1.027803    1.016265    1.019974    1.000296    1.015797   \n",
      "min     -2.335437   -1.267069   -0.982843   -1.313990   -2.707379   -3.883072   \n",
      "25%     -0.768994   -0.829872   -0.637962   -0.755697   -0.488039    0.197588   \n",
      "50%      0.338718   -0.329213   -0.523001   -0.440915    0.297977    0.374827   \n",
      "75%      0.911243    0.674172    1.661245    1.530926    0.806576    0.429868   \n",
      "max      1.117494    3.960518    1.661245    1.798194    1.638828    0.441052   \n",
      "\n",
      "            LSTAT           Y  \n",
      "count  379.000000  379.000000  \n",
      "mean     0.018418   22.609499  \n",
      "std      1.015377    9.389647  \n",
      "min     -1.531127    5.000000  \n",
      "25%     -0.828856   16.700000  \n",
      "50%     -0.161629   21.100000  \n",
      "75%      0.647173   25.750000  \n",
      "max      3.409999   50.000000  \n",
      "(379, 13)\n",
      "(379,)\n",
      "               0           1           2           3           4           5   \\\n",
      "count  127.000000  127.000000  127.000000  127.000000  127.000000  127.000000   \n",
      "mean    -0.058575   -0.007327   -0.107939   -0.086410   -0.085871   -0.096098   \n",
      "std      0.769837    1.005445    0.945672    0.839435    1.003998    0.998196   \n",
      "min     -0.417173   -0.487722   -1.557842   -0.272599   -1.431329   -3.058221   \n",
      "25%     -0.410832   -0.487722   -0.891036   -0.272599   -0.947582   -0.567918   \n",
      "50%     -0.398269   -0.487722   -0.375976   -0.272599   -0.299707   -0.127698   \n",
      "75%     -0.242900   -0.219475    1.015999   -0.272599    0.434551    0.283316   \n",
      "max      3.966816    3.589637    2.117615    3.668398    2.732346    3.476688   \n",
      "\n",
      "               6           7           8           9           10          11  \\\n",
      "count  127.000000  127.000000  127.000000  127.000000  127.000000  127.000000   \n",
      "mean    -0.114581    0.003845   -0.129240   -0.130670   -0.057350    0.047105   \n",
      "std      1.042254    0.920171    0.946051    0.933732    1.004824    0.957787   \n",
      "min     -2.225199   -1.263551   -0.982843   -1.308051   -2.707379   -3.907193   \n",
      "25%     -1.240171   -0.762417   -0.637962   -0.785394   -0.765457    0.246544   \n",
      "50%      0.111130   -0.202052   -0.523001   -0.601276    0.113032    0.396098   \n",
      "75%      0.898797    0.604198   -0.350561    0.072833    0.806576    0.441052   \n",
      "max      1.117494    3.287300    1.661245    1.530926    1.268938    0.441052   \n",
      "\n",
      "               12  \n",
      "count  127.000000  \n",
      "mean    -0.054965  \n",
      "std      0.958559  \n",
      "min     -1.496084  \n",
      "25%     -0.678870  \n",
      "50%     -0.283580  \n",
      "75%      0.389254  \n",
      "max      3.548771  \n",
      "(127, 13)\n"
     ]
    }
   ],
   "source": [
    "# Loading Datasets\n",
    "\n",
    "train_dataset = np.genfromtxt(\"../training_boston_x_y_train.csv\", names = True, delimiter = \",\")\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "print(train_df.describe())\n",
    "num_cols = len(train_df.columns)\n",
    "X_train = train_df.values[:, 0: num_cols -1 ]\n",
    "Y_train = train_df.values[:, num_cols - 1]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "# print(X_train)\n",
    "# print(Y_train)\n",
    "\n",
    "test_dataset = np.genfromtxt(\"../test_boston_x_test.csv\", names = None, delimiter = \",\")\n",
    "test_df = pd.DataFrame(test_dataset)\n",
    "print(test_df.describe())\n",
    "num_cols = len(test_df.columns)\n",
    "X_test = test_df.values\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "# print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379, 105)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns = np.arange(14)\n",
    "for i in range(13):\n",
    "    for j in range(i, 13):\n",
    "        train_df[str(i)+\"_\"+str(j)] = train_df[i]*train_df[j]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\"\"\"\n",
    "    n = 10\n",
    "    sub_Size = 0.7\n",
    "    Y_pred = np.zeros(len(X_test))\n",
    "    Y_train_pred = np.zeros(int(sub_Size * len(X_train)))\n",
    "    num_s = 0\n",
    "    for i in range(n):\n",
    "        X1, X2, Y1, Y2 = model_selection.train_test_split(X_train, Y_train, test_size = 1 - sub_Size)\n",
    "        model = GradientBoostingRegressor(loss = 'huber', subsample = 0.9)\n",
    "        model.fit(X1, Y1)\n",
    "        #print(\"Training Score : \", model.score(X_train, Y_train))\n",
    "        score = model.score(X1, Y1)\n",
    "        if score >= 0:\n",
    "            print(num_s,\" Training Score : \", score)\n",
    "            Y_i_pred = model.predict(X_test)\n",
    "            Y_pred += Y_i_pred\n",
    "            Y_train_i_pred = model.predict(X1)\n",
    "            Y_train_pred += Y_train_i_pred\n",
    "            num_s += 1\n",
    "    Y_pred /= num_s\n",
    "    Y_train_pred /= num_s          \n",
    "    \n",
    "    #print(Y_pred, pred)\n",
    "    return Y_train_pred, Y_pred\n",
    "\"\"\"\n",
    "def add_more_features (X_train, imp_cols_indices = []):\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    num_f = len(X_train_df.columns)\n",
    "    col_names = X_train_df.columns\n",
    "    \n",
    "    if len(imp_cols_indices) == 0 :\n",
    "        imp_cols_indices = np.arange(num_f)\n",
    "    new_df = X_train_df.copy()\n",
    "    num_imp_cols = len(imp_cols_indices)\n",
    "    \n",
    "    pow = 3\n",
    "    \n",
    "    while pow < 10 :\n",
    "        for i1 in range(num_imp_cols) :\n",
    "            i = imp_cols_indices[i1]\n",
    "            new_df[ str(col_names[i]) + \"_pow_\" + str(pow) ] = X_train_df[i] ** pow\n",
    "        pow += 1\n",
    "    \n",
    "    for i1 in range(num_imp_cols) :\n",
    "        for j1 in range(i1, num_imp_cols):\n",
    "            i = imp_cols_indices[i1]\n",
    "            j = imp_cols_indices[j1]\n",
    "            new_df[ str(col_names[i]) + \"_\" + str(col_names[j])] = X_train_df[i] * X_train_df[j]\n",
    "            new_df[ str(col_names[i])*2 + \"_\" + str(col_names[j])] = X_train_df[i]**2 * X_train_df[j]\n",
    "            new_df[ str(col_names[i]) + \"_\" + str(col_names[j])*2 ] = X_train_df[i] * X_train_df[j]**2\n",
    "            new_df[ str(col_names[i])*2 + \"_\" + str(col_names[j])*2 ] = X_train_df[i]**2 * X_train_df[j]**2\n",
    "            \n",
    "            new_df[ str(col_names[i])*3 + \"_\" + str(col_names[j])] = X_train_df[i]**3 * X_train_df[j]\n",
    "            new_df[ str(col_names[i])*3 + \"_\" + str(col_names[j])*2] = X_train_df[i]**3 * X_train_df[j]**2\n",
    "            new_df[ str(col_names[i])*3 + \"_\" + str(col_names[j])*3 ] = X_train_df[i]**3 * X_train_df[j]**3\n",
    "            new_df[ str(col_names[i])*2 + \"_\" + str(col_names[j])*3 ] = X_train_df[i]**2 * X_train_df[j]**3\n",
    "            new_df[ str(col_names[i]) + \"_\" + str(col_names[j])*3 ] = X_train_df[i] * X_train_df[j]**3\n",
    "            \n",
    "            \n",
    "            \n",
    "    print(new_df.describe())\n",
    "    return new_df.values\n",
    "\n",
    "def add_cube_features (X_train):\n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    num_f = len(X_train_df.columns)\n",
    "    col_names = X_train_df.columns\n",
    "    \n",
    "\n",
    "    imp_cols_indices = np.arange(num_f)\n",
    "    new_df = X_train_df.copy()\n",
    "    num_imp_cols = len(imp_cols_indices)\n",
    "    \n",
    "    pow = 3\n",
    "    \n",
    "    for i1 in range(num_imp_cols) :\n",
    "        i = imp_cols_indices[i1]\n",
    "        new_df[ str(col_names[i]) + \"_pow_\" + str(pow) ] = X_train_df[i] ** 3\n",
    "    \"\"\"\n",
    "    \n",
    "    for i1 in range(num_imp_cols) :\n",
    "        for j1 in range(i1, num_imp_cols):\n",
    "            i = imp_cols_indices[i1]\n",
    "            j = imp_cols_indices[j1]\n",
    "            new_df[ str(col_names[i]) + \"_\" + str(col_names[j])] = X_train_df[i] * X_train_df[j]\n",
    "    \"\"\"\n",
    "    #print(new_df.describe())\n",
    "    return new_df.values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2           3           4           5  \\\n",
      "count  379.000000  379.000000  379.000000  379.000000  379.000000  379.000000   \n",
      "mean     0.019628    0.002455    0.036170    0.028955    0.028775    0.032202   \n",
      "std      1.067490    1.000813    1.017497    1.048995    0.999656    1.001174   \n",
      "min     -0.417713   -0.487722   -1.516987   -0.272599   -1.465882   -3.880249   \n",
      "25%     -0.408171   -0.487722   -0.867691   -0.272599   -0.878475   -0.571480   \n",
      "50%     -0.383729   -0.487722   -0.180458   -0.272599   -0.144217   -0.103479   \n",
      "75%      0.055208    0.156071    1.015999   -0.272599    0.628913    0.529069   \n",
      "max      9.941735    3.804234    2.422565    3.668398    2.732346    3.555044   \n",
      "\n",
      "                6           7           8           9      ...       \\\n",
      "count  379.000000  379.000000  379.000000  379.000000      ...        \n",
      "mean     0.038395   -0.001288    0.043307    0.043786      ...        \n",
      "std      0.985209    1.027803    1.016265    1.019974      ...        \n",
      "min     -2.335437   -1.267069   -0.982843   -1.313990      ...        \n",
      "25%     -0.768994   -0.829872   -0.637962   -0.755697      ...        \n",
      "50%      0.338718   -0.329213   -0.523001   -0.440915      ...        \n",
      "75%      0.911243    0.674172    1.661245    1.530926      ...        \n",
      "max      1.117494    3.960518    1.661245    1.798194      ...        \n",
      "\n",
      "       1111_121212       12_12     1212_12     12_1212     1212_1212  \\\n",
      "count   379.000000  379.000000  379.000000  379.000000  3.790000e+02   \n",
      "mean      4.301064    1.028610    0.884369    0.884369  3.328985e+00   \n",
      "std      30.402724    1.508958    4.343654    4.343654  1.191946e+01   \n",
      "min      -0.682695    0.000003   -3.589498   -3.589498  1.124261e-11   \n",
      "25%      -0.069126    0.134259   -0.569432   -0.569432  1.807728e-02   \n",
      "50%      -0.000509    0.572306   -0.004222   -0.004222  3.275341e-01   \n",
      "75%       0.031155    1.246537    0.271058    0.271058  1.553855e+00   \n",
      "max     364.588581   11.628092   39.651778   39.651778  1.352125e+02   \n",
      "\n",
      "          121212_12  121212_1212  121212_121212  1212_121212     12_121212  \n",
      "count  3.790000e+02   379.000000   3.790000e+02   379.000000  3.790000e+02  \n",
      "mean   3.328985e+00     6.671759   1.959966e+01     6.671759  3.328985e+00  \n",
      "std    1.191946e+01    36.376241   1.131155e+02    36.376241  1.191946e+01  \n",
      "min    1.124261e-11    -8.415041   3.769648e-17    -8.415041  1.124261e-11  \n",
      "25%    1.807728e-02    -0.391209   2.440920e-03    -0.391209  1.807728e-02  \n",
      "50%    3.275341e-01    -0.000110   1.874497e-01    -0.000110  3.275341e-01  \n",
      "75%    1.553855e+00     0.113528   1.936939e+00     0.113528  1.553855e+00  \n",
      "max    1.352125e+02   461.074501   1.572263e+03   461.074501  1.352125e+02  \n",
      "\n",
      "[8 rows x 911 columns]\n",
      "                0           1           2           3           4           5  \\\n",
      "count  127.000000  127.000000  127.000000  127.000000  127.000000  127.000000   \n",
      "mean    -0.058575   -0.007327   -0.107939   -0.086410   -0.085871   -0.096098   \n",
      "std      0.769837    1.005445    0.945672    0.839435    1.003998    0.998196   \n",
      "min     -0.417173   -0.487722   -1.557842   -0.272599   -1.431329   -3.058221   \n",
      "25%     -0.410832   -0.487722   -0.891036   -0.272599   -0.947582   -0.567918   \n",
      "50%     -0.398269   -0.487722   -0.375976   -0.272599   -0.299707   -0.127698   \n",
      "75%     -0.242900   -0.219475    1.015999   -0.272599    0.434551    0.283316   \n",
      "max      3.966816    3.589637    2.117615    3.668398    2.732346    3.476688   \n",
      "\n",
      "                6           7           8           9      ...       \\\n",
      "count  127.000000  127.000000  127.000000  127.000000      ...        \n",
      "mean    -0.114581    0.003845   -0.129240   -0.130670      ...        \n",
      "std      1.042254    0.920171    0.946051    0.933732      ...        \n",
      "min     -2.225199   -1.263551   -0.982843   -1.308051      ...        \n",
      "25%     -1.240171   -0.762417   -0.637962   -0.785394      ...        \n",
      "50%      0.111130   -0.202052   -0.523001   -0.601276      ...        \n",
      "75%      0.898797    0.604198   -0.350561    0.072833      ...        \n",
      "max      1.117494    3.287300    1.661245    1.530926      ...        \n",
      "\n",
      "       1111_121212       12_12     1212_12     12_1212     1212_1212  \\\n",
      "count   127.000000  127.000000  127.000000  127.000000  1.270000e+02   \n",
      "mean      1.901513    0.914621    0.961671    0.961671  3.916901e+00   \n",
      "std      14.671212    1.762049    5.529101    5.529101  1.719087e+01   \n",
      "min      -0.610603    0.000006   -3.348634   -3.348634  3.176792e-11   \n",
      "25%      -0.036554    0.091965   -0.312884   -0.312884  8.469042e-03   \n",
      "50%      -0.001007    0.311722   -0.022805   -0.022805  9.717084e-02   \n",
      "75%       0.020670    1.015787    0.060530    0.060530  1.031933e+00   \n",
      "max     158.403783   12.593774   44.692418   44.692418  1.586032e+02   \n",
      "\n",
      "          121212_12  121212_1212  121212_121212  1212_121212     12_121212  \n",
      "count  1.270000e+02   127.000000   1.270000e+02   127.000000  1.270000e+02  \n",
      "mean   3.916901e+00     9.682401   3.125505e+01     9.682401  3.916901e+00  \n",
      "std    1.719087e+01    57.294154   1.942109e+02    57.294154  1.719087e+01  \n",
      "min    3.176792e-11    -7.495134   1.790536e-16    -7.495134  3.176792e-11  \n",
      "25%    8.469042e-03    -0.144214   7.809781e-04    -0.144214  8.469042e-03  \n",
      "50%    9.717084e-02    -0.001834   3.029033e-02    -0.001834  9.717084e-02  \n",
      "75%    1.031933e+00     0.009723   1.048446e+00     0.009723  1.031933e+00  \n",
      "max    1.586032e+02   562.846230   1.997412e+03   562.846230  1.586032e+02  \n",
      "\n",
      "[8 rows x 911 columns]\n",
      "(379, 911)\n",
      "(127, 911)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing as pps\n",
    "\n",
    "X_train_new = add_more_features(X_train)\n",
    "X_test_new = add_more_features(X_test)\n",
    "\n",
    "#X_train_new = add_more_features(X_train_new)\n",
    "#X_test_new = add_more_features(X_test_new)\n",
    "\n",
    "std_scaler = pps.StandardScaler()\n",
    "std_scaler.fit(X_train_new)\n",
    "\n",
    "X_train_scaled = std_scaler.transform(X_train_new)\n",
    "X_test_scaled = std_scaler.transform(X_test_new)\n",
    "\n",
    "import copy\n",
    "X_train_original = copy.deepcopy(X_train)\n",
    "X_test_original = copy.deepcopy(X_test)\n",
    "\n",
    "X_train = X_train_scaled\n",
    "X_test = X_test_scaled\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 200)\n",
      "(127, 200)\n"
     ]
    }
   ],
   "source": [
    "# extract best features\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(200)\n",
    "pca.fit(np.concatenate((X_train, X_test), axis = 0))\n",
    "\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.999382177435\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingRegressor(loss = 'huber', subsample = 1, n_estimators = 100, max_depth =  5 )\n",
    "model.fit(X_train, Y_train)\n",
    "print(\"Training Score: \", model.score(X_train, Y_train))\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"boston_dataset_pred.csv\", Y_pred, '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
