{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent - Boston Dataset\n",
    "Boston dataset is one of the datasets available in sklearn.\n",
    "You are given a Training dataset csv file with X train and Y train data. As studied in lecture, your task is to come up with Gradient Descent algorithm and thus predictions for the test dataset given.\n",
    "Your task is to:\n",
    "    1. Code Gradient Descent for N features and come with predictions.\n",
    "    2. Try and test with various combinations of learning rates and number of iterations.\n",
    "    3. Try using Feature Scaling, and see if it helps you in getting better results. \n",
    "Read Instructions carefully -\n",
    "    1. Use Gradient Descent as a training algorithm and submit results predicted.\n",
    "    2. Files are in csv format, you can use genfromtxt function in numpy to load data from csv file. Similarly you can use savetxt function to save data into a file.\n",
    "    3. Submit a csv file with only predictions for X test data. File name should not have spaces. File should not have any headers and should only have one column i.e. predictions. Also predictions shouldn't be in exponential form. \n",
    "    4. Your score is based on coefficient of determination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 T            V           AP           RH           EP\n",
      "count  7176.000000  7176.000000  7176.000000  7176.000000  7176.000000\n",
      "mean     19.629712    54.288154  1013.263032    73.275818   454.431293\n",
      "std       7.475256    12.751468     5.964863    14.625093    17.134571\n",
      "min       1.810000    25.360000   992.890000    25.560000   420.260000\n",
      "25%      13.470000    41.740000  1009.010000    63.202500   439.737500\n",
      "50%      20.315000    52.050000  1012.910000    74.895000   451.740000\n",
      "75%      25.720000    66.540000  1017.302500    84.925000   468.667500\n",
      "max      35.770000    81.560000  1033.300000   100.160000   495.760000\n",
      "(7176, 4)\n",
      "(7176,)\n",
      "                0            1            2            3\n",
      "count  2392.00000  2392.000000  2392.000000  2392.000000\n",
      "mean     19.71579    54.358754  1013.247216    73.408457\n",
      "std       7.38488    12.578763     5.861068    14.528135\n",
      "min       3.38000    25.360000   993.740000    26.670000\n",
      "25%      13.66000    41.730000  1009.300000    63.615000\n",
      "50%      20.45000    52.750000  1013.025000    75.090000\n",
      "75%      25.67250    66.490000  1017.172500    84.497500\n",
      "max      37.11000    80.250000  1033.290000   100.130000\n",
      "(2392, 4)\n"
     ]
    }
   ],
   "source": [
    "# Loading Datasets\n",
    "\n",
    "train_dataset = np.genfromtxt(\"../training_ccpp_x_y_train.csv\", names = True, delimiter = \",\")\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "print(train_df.describe())\n",
    "num_cols = len(train_df.columns)\n",
    "X_train = train_df.values[:, 0: num_cols -1 ]\n",
    "Y_train = train_df.values[:, num_cols - 1]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "# print(X_train)\n",
    "# print(Y_train)\n",
    "\n",
    "test_dataset = np.genfromtxt(\"../test_ccpp_x_test.csv\", names = None, delimiter = \",\")\n",
    "test_df = pd.DataFrame(test_dataset)\n",
    "print(test_df.describe())\n",
    "num_cols = len(test_df.columns)\n",
    "X_test = test_df.values\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "# print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\"\"\"\n",
    "    n = 10\n",
    "    sub_Size = 0.7\n",
    "    Y_pred = np.zeros(len(X_test))\n",
    "    Y_train_pred = np.zeros(int(sub_Size * len(X_train)))\n",
    "    num_s = 0\n",
    "    for i in range(n):\n",
    "        X1, X2, Y1, Y2 = model_selection.train_test_split(X_train, Y_train, test_size = 1 - sub_Size)\n",
    "        model = GradientBoostingRegressor(loss = 'huber', subsample = 0.9)\n",
    "        model.fit(X1, Y1)\n",
    "        #print(\"Training Score : \", model.score(X_train, Y_train))\n",
    "        score = model.score(X1, Y1)\n",
    "        if score >= 0:\n",
    "            print(num_s,\" Training Score : \", score)\n",
    "            Y_i_pred = model.predict(X_test)\n",
    "            Y_pred += Y_i_pred\n",
    "            Y_train_i_pred = model.predict(X1)\n",
    "            Y_train_pred += Y_train_i_pred\n",
    "            num_s += 1\n",
    "    Y_pred /= num_s\n",
    "    Y_train_pred /= num_s          \n",
    "    \n",
    "    #print(Y_pred, pred)\n",
    "    return Y_train_pred, Y_pred\n",
    "\"\"\"\n",
    "def add_more_features (X_train, imp_cols_indices = []):\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    num_f = len(X_train_df.columns)\n",
    "    col_names = X_train_df.columns\n",
    "    \n",
    "    if len(imp_cols_indices) == 0 :\n",
    "        imp_cols_indices = np.arange(num_f)\n",
    "    new_df = X_train_df.copy()\n",
    "    num_imp_cols = len(imp_cols_indices)\n",
    "    \n",
    "    pow = 3\n",
    "    \n",
    "    while pow < 10 :\n",
    "        for i1 in range(num_imp_cols) :\n",
    "            i = imp_cols_indices[i1]\n",
    "            new_df[ str(col_names[i]) + \"_pow_\" + str(pow) ] = X_train_df[i] ** pow\n",
    "        pow += 1\n",
    "    \n",
    "    for i1 in range(num_imp_cols) :\n",
    "        for j1 in range(i1, num_imp_cols):\n",
    "            i = imp_cols_indices[i1]\n",
    "            j = imp_cols_indices[j1]\n",
    "            new_df[ str(col_names[i]) + \"_\" + str(col_names[j])] = X_train_df[i] * X_train_df[j]\n",
    "            new_df[ str(col_names[i])*2 + \"_\" + str(col_names[j])] = X_train_df[i]**2 * X_train_df[j]\n",
    "            new_df[ str(col_names[i]) + \"_\" + str(col_names[j])*2 ] = X_train_df[i] * X_train_df[j]**2\n",
    "            new_df[ str(col_names[i])*2 + \"_\" + str(col_names[j])*2 ] = X_train_df[i]**2 * X_train_df[j]**2\n",
    "            \n",
    "            new_df[ str(col_names[i])*3 + \"_\" + str(col_names[j])] = X_train_df[i]**3 * X_train_df[j]\n",
    "            new_df[ str(col_names[i])*3 + \"_\" + str(col_names[j])*2] = X_train_df[i]**3 * X_train_df[j]**2\n",
    "            new_df[ str(col_names[i])*3 + \"_\" + str(col_names[j])*3 ] = X_train_df[i]**3 * X_train_df[j]**3\n",
    "            new_df[ str(col_names[i])*2 + \"_\" + str(col_names[j])*3 ] = X_train_df[i]**2 * X_train_df[j]**3\n",
    "            new_df[ str(col_names[i]) + \"_\" + str(col_names[j])*3 ] = X_train_df[i] * X_train_df[j]**3\n",
    "            \n",
    "            \n",
    "            \n",
    "    print(new_df.describe())\n",
    "    return new_df.values\n",
    "\n",
    "def add_cube_features (X_train):\n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    num_f = len(X_train_df.columns)\n",
    "    col_names = X_train_df.columns\n",
    "    \n",
    "\n",
    "    imp_cols_indices = np.arange(num_f)\n",
    "    new_df = X_train_df.copy()\n",
    "    num_imp_cols = len(imp_cols_indices)\n",
    "    \n",
    "    pow = 3\n",
    "    \n",
    "    for i1 in range(num_imp_cols) :\n",
    "        i = imp_cols_indices[i1]\n",
    "        new_df[ str(col_names[i]) + \"_pow_\" + str(pow) ] = X_train_df[i] ** 3\n",
    "    \"\"\"\n",
    "    \n",
    "    for i1 in range(num_imp_cols) :\n",
    "        for j1 in range(i1, num_imp_cols):\n",
    "            i = imp_cols_indices[i1]\n",
    "            j = imp_cols_indices[j1]\n",
    "            new_df[ str(col_names[i]) + \"_\" + str(col_names[j])] = X_train_df[i] * X_train_df[j]\n",
    "    \"\"\"\n",
    "    #print(new_df.describe())\n",
    "    return new_df.values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0            1            2            3       0_pow_3  \\\n",
      "count  7176.000000  7176.000000  7176.000000  7176.000000   7176.000000   \n",
      "mean     19.629712    54.288154  1013.263032    73.275818  10800.841722   \n",
      "std       7.475256    12.751468     5.964863    14.625093   9563.618433   \n",
      "min       1.810000    25.360000   992.890000    25.560000      5.929741   \n",
      "25%      13.470000    41.740000  1009.010000    63.202500   2444.008923   \n",
      "50%      20.315000    52.050000  1012.910000    74.895000   8383.986280   \n",
      "75%      25.720000    66.540000  1017.302500    84.925000  17014.253248   \n",
      "max      35.770000    81.560000  1033.300000   100.160000  45767.461033   \n",
      "\n",
      "             1_pow_3       2_pow_3       3_pow_3       0_pow_4       1_pow_4  \\\n",
      "count    7176.000000  7.176000e+03  7.176000e+03  7.176000e+03  7.176000e+03   \n",
      "mean   186897.600572  1.040427e+09  4.391549e+05  2.795492e+05  1.169357e+07   \n",
      "std    122952.564978  1.840424e+07  2.296969e+05  3.025765e+05  9.738584e+06   \n",
      "min     16309.766656  9.788213e+08  1.669870e+04  1.073283e+01  4.136157e+05   \n",
      "25%     72720.580024  1.027274e+09  2.524660e+05  3.292080e+04  3.035357e+06   \n",
      "50%    141013.990125  1.039232e+09  4.201056e+05  1.703207e+05  7.339778e+06   \n",
      "75%    294610.614264  1.052811e+09  6.125008e+05  4.376066e+05  1.960339e+07   \n",
      "max    542539.860416  1.103264e+09  1.004808e+06  1.637102e+06  4.424955e+07   \n",
      "\n",
      "           ...              2_333           3_3          33_3          3_33  \\\n",
      "count      ...       7.176000e+03   7176.000000  7.176000e+03  7.176000e+03   \n",
      "mean       ...       4.450903e+08   5583.209039  4.391549e+05  4.391549e+05   \n",
      "std        ...       2.328294e+08   2069.245715  2.296969e+05  2.296969e+05   \n",
      "min        ...       1.696988e+07    653.313600  1.669870e+04  1.669870e+04   \n",
      "25%        ...       2.558461e+08   3994.556175  2.524660e+05  2.524660e+05   \n",
      "50%        ...       4.269695e+08   5609.261050  4.201056e+05  4.201056e+05   \n",
      "75%        ...       6.216141e+08   7212.255700  6.125008e+05  6.125008e+05   \n",
      "max        ...       1.022980e+09  10032.025600  1.004808e+06  1.004808e+06   \n",
      "\n",
      "              33_33         333_3        333_33       333_333        33_333  \\\n",
      "count  7.176000e+03  7.176000e+03  7.176000e+03  7.176000e+03  7.176000e+03   \n",
      "mean   3.545340e+07  3.545340e+07  2.924449e+09  2.456103e+11  2.924449e+09   \n",
      "std    2.350841e+07  2.350841e+07  2.324267e+09  2.261685e+11  2.324267e+09   \n",
      "min    4.268187e+05  4.268187e+05  1.090948e+07  2.788464e+08  1.090948e+07   \n",
      "25%    1.595648e+07  1.595648e+07  1.008490e+09  6.373908e+10  1.008490e+09   \n",
      "50%    3.146381e+07  3.146381e+07  2.356482e+09  1.764887e+11  2.356482e+09   \n",
      "75%    5.201663e+07  5.201663e+07  4.417513e+09  3.751573e+11  4.417513e+09   \n",
      "max    1.006415e+08  1.006415e+08  1.008026e+10  1.009638e+12  1.008026e+10   \n",
      "\n",
      "              3_333  \n",
      "count  7.176000e+03  \n",
      "mean   3.545340e+07  \n",
      "std    2.350841e+07  \n",
      "min    4.268187e+05  \n",
      "25%    1.595648e+07  \n",
      "50%    3.146381e+07  \n",
      "75%    5.201663e+07  \n",
      "max    1.006415e+08  \n",
      "\n",
      "[8 rows x 122 columns]\n",
      "                0            1            2            3       0_pow_3  \\\n",
      "count  2392.00000  2392.000000  2392.000000  2392.000000   2392.000000   \n",
      "mean     19.71579    54.358754  1013.247216    73.408457  10822.295942   \n",
      "std       7.38488    12.578763     5.861068    14.528135   9428.043104   \n",
      "min       3.38000    25.360000   993.740000    26.670000     38.614472   \n",
      "25%      13.66000    41.730000  1009.300000    63.615000   2548.895896   \n",
      "50%      20.45000    52.750000  1013.025000    75.090000   8552.241125   \n",
      "75%      25.67250    66.490000  1017.172500    84.497500  16920.162305   \n",
      "max      37.11000    80.250000  1033.290000   100.130000  51106.114431   \n",
      "\n",
      "             1_pow_3       2_pow_3       3_pow_3       0_pow_4       1_pow_4  \\\n",
      "count    2392.000000  2.392000e+03  2.392000e+03  2.392000e+03  2.392000e+03   \n",
      "mean   186781.193471  1.040375e+09  4.405771e+05  2.790178e+05  1.165360e+07   \n",
      "std    120862.471910  1.807548e+07  2.268864e+05  2.982010e+05  9.546235e+06   \n",
      "min     16309.766656  9.813373e+08  1.897007e+04  1.305169e+02  4.136157e+05   \n",
      "25%     72668.363268  1.028160e+09  2.574415e+05  3.481792e+04  3.032452e+06   \n",
      "50%    146780.171875  1.039586e+09  4.233956e+05  1.748933e+05  7.742654e+06   \n",
      "75%    293946.977449  1.052407e+09  6.032976e+05  4.343829e+05  1.954453e+07   \n",
      "max    516815.015625  1.103232e+09  1.003905e+06  1.896548e+06  4.147441e+07   \n",
      "\n",
      "           ...              2_333           3_3          33_3          3_33  \\\n",
      "count      ...       2.392000e+03   2392.000000  2.392000e+03  2.392000e+03   \n",
      "mean       ...       4.465138e+08   5599.780087  4.405771e+05  4.405771e+05   \n",
      "std        ...       2.299800e+08   2047.547505  2.268864e+05  2.268864e+05   \n",
      "min        ...       1.927417e+07    711.288900  1.897007e+04  1.897007e+04   \n",
      "25%        ...       2.616754e+08   4046.868300  2.574415e+05  2.574415e+05   \n",
      "50%        ...       4.294373e+08   5638.508100  4.233956e+05  4.233956e+05   \n",
      "75%        ...       6.122938e+08   7139.827675  6.032976e+05  6.032976e+05   \n",
      "max        ...       1.017845e+09  10026.016900  1.003905e+06  1.003905e+06   \n",
      "\n",
      "              33_33         333_3        333_33       333_333        33_333  \\\n",
      "count  2.392000e+03  2.392000e+03  2.392000e+03  2.392000e+03  2.392000e+03   \n",
      "mean   3.554824e+07  3.554824e+07  2.928721e+09  2.455641e+11  2.928721e+09   \n",
      "std    2.321103e+07  2.321103e+07  2.295725e+09  2.235737e+11  2.295725e+09   \n",
      "min    5.059319e+05  5.059319e+05  1.349320e+07  3.598637e+08  1.349320e+07   \n",
      "25%    1.637714e+07  1.637714e+07  1.041832e+09  6.627616e+10  1.041832e+09   \n",
      "50%    3.179277e+07  3.179277e+07  2.387319e+09  1.792638e+11  2.387319e+09   \n",
      "75%    5.097714e+07  5.097714e+07  4.307442e+09  3.639681e+11  4.307442e+09   \n",
      "max    1.005210e+08  1.005210e+08  1.006517e+10  1.007825e+12  1.006517e+10   \n",
      "\n",
      "              3_333  \n",
      "count  2.392000e+03  \n",
      "mean   3.554824e+07  \n",
      "std    2.321103e+07  \n",
      "min    5.059319e+05  \n",
      "25%    1.637714e+07  \n",
      "50%    3.179277e+07  \n",
      "75%    5.097714e+07  \n",
      "max    1.005210e+08  \n",
      "\n",
      "[8 rows x 122 columns]\n",
      "(7176, 122)\n",
      "(2392, 122)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing as pps\n",
    "\n",
    "X_train_new = add_more_features(X_train)\n",
    "X_test_new = add_more_features(X_test)\n",
    "\n",
    "#X_train_new = add_more_features(X_train_new)\n",
    "#X_test_new = add_more_features(X_test_new)\n",
    "\n",
    "std_scaler = pps.StandardScaler()\n",
    "std_scaler.fit(X_train_new)\n",
    "\n",
    "X_train_scaled = std_scaler.transform(X_train_new)\n",
    "X_test_scaled = std_scaler.transform(X_test_new)\n",
    "\n",
    "import copy\n",
    "X_train_original = copy.deepcopy(X_train)\n",
    "X_test_original = copy.deepcopy(X_test)\n",
    "\n",
    "X_train = X_train_scaled\n",
    "X_test = X_test_scaled\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7176, 122)\n",
      "(2392, 122)\n"
     ]
    }
   ],
   "source": [
    "# extract best features\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(np.concatenate((X_train, X_test), axis = 0))\n",
    "\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.999996326189\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingRegressor(loss = 'huber', subsample = 0.5, n_estimators = 1000, max_depth =  10, warm_start = True, max_leaf_nodes=None, min_samples_leaf=1)\n",
    "model.fit(X_train, Y_train)\n",
    "print(\"Training Score: \", model.score(X_train, Y_train))\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.998944045823\n",
      "Training Score:  0.999042356103\n",
      "Training Score:  0.999112198717\n",
      "Training Score:  0.999020818777\n",
      "Training Score:  0.999048205321\n",
      "Training Score:  0.999050261792\n",
      "Training Score:  0.99901812032\n",
      "Training Score:  0.999069745974\n",
      "Training Score:  0.999112615863\n",
      "Training Score:  0.999007934006\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "for i in range(10) :\n",
    "    Y_test = copy.copy(Y_pred)\n",
    "    model = GradientBoostingRegressor(loss = 'huber', subsample = 0.8, n_estimators = 300, max_depth =  8, warm_start = True, max_leaf_nodes=None, min_samples_leaf=1)\n",
    "    model.fit(np.concatenate((X_train, X_test)), np.concatenate((Y_train, Y_test)))\n",
    "    print(\"Training Score: \", model.score(np.concatenate((X_train, X_test)), np.concatenate((Y_train, Y_test))))\n",
    "    Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"ccpp_dataset_pred.csv\", Y_pred, '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
